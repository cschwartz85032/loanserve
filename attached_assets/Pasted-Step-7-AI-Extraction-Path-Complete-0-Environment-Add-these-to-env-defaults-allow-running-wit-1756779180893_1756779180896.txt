Step 7 â€“ AI Extraction Path (Complete)
0) Environment

Add these to .env (defaults allow running with the built-in mock):

# LLM provider: mock | openai
LLM_PROVIDER=mock
LLM_MODEL=gpt-4o-mini
OPENAI_API_KEY=

# AI runtime knobs
AI_TEMPERATURE=0.0
AI_MAX_TOKENS=2000
AI_REQUEST_TIMEOUT_MS=60000
AI_SLICE_TARGET_CHARS=8000
AI_MIN_CONFIDENCE=0.60      # below this -> reject
AI_ACCEPT_CONFIDENCE=0.80   # at/above -> accept

1) Keysets per Doc Type (what AI should extract)

File: src/ai/keysets.ts

// The authoritative keys we ask the AI to extract per docType.
// Keep this aligned with your prompt packs and Authority Matrix.
//
// You can add more keys anytime; the AI runner will only attempt
// keys not already satisfied by deterministic extraction.

export const DOC_KEYSETS: Record<string, string[]> = {
  NOTE: [
    "NoteAmount","InterestRate","AmortTermMonths",
    "FirstPaymentDate","MaturityDate","LateChargePct","LateChargeGraceDays",
    "BorrowerFullName"
  ],
  CD: [
    "TotalLoanAmount","PAndIAmount","EscrowRequired",
    "TaxEscrowMonthly","InsuranceEscrowMonthly","HOICarrier","HOIPolicyNumber",
    "PropertyAddress"
  ],
  HOI: [
    "HomeownersInsCarrier","HOIPolicyNumber","HOIEffectiveDate","HOIExpirationDate","HOIPhone","HOIEmail"
  ],
  FLOOD: [
    "FloodZone","FloodInsRequired","DeterminationIdentifier"
  ],
  APPRAISAL: [
    "AppraisalDate","AppraisedValue","AppraisalFormType"
  ],
  DEED: [
    // no AI by default for deed; deterministic MIN finder + HITL if needed
  ]
};

2) OCR Slicing (safe chunks for the LLM)

File: src/utils/ocr-slices.ts

import { getBytes, getText } from "../utils/storage";

/**
 * Return text slices for a doc.
 * Strategy: use pre-reflowed text if available; otherwise read page-text files
 * (you may extend this to page-by-page slicing if you store per-page text).
 */
export async function getDocTextSlices(docId: string, targetChars = Number(process.env.AI_SLICE_TARGET_CHARS || "8000")): Promise<{slice: string, idx: number}[]> {
  // default: load a single text file text/{docId}.txt
  try {
    const text = await getText(`text/${docId}.txt`);
    return chunk(text, targetChars);
  } catch {
    // If not found, fall back to empty; worker will skip AI
    return [];
  }
}

function chunk(s: string, n: number): {slice: string, idx: number}[] {
  if (s.length <= n) return [{ slice: s, idx: 0 }];
  const out: {slice: string, idx: number}[] = [];
  let i = 0, idx = 0;
  while (i < s.length) {
    out.push({ slice: s.slice(i, i + n), idx });
    i += n;
    idx += 1;
  }
  return out;
}

3) LLM Provider (mock + OpenAI)

File: src/ai/llm.ts

type GenArgs = { prompt: string; model?: string; temperature?: number; maxTokens?: number; timeoutMs?: number; };
export type LlmOutput = { text: string };

export interface LlmProvider {
  generate(args: GenArgs): Promise<LlmOutput>;
}

class MockProvider implements LlmProvider {
  async generate({ prompt }: GenArgs): Promise<LlmOutput> {
    // Naive mock that returns a minimally valid JSON skeleton derived from prompt docType.
    // This is ONLY for local/dev. Real provider is below.
    const type = /"docType"\s*:\s*"(NOTE|CD|HOI|FLOOD|APPRAISAL)"/i.exec(prompt)?.[1] || "NOTE";
    const pver = `v${new Date().toISOString().slice(0,10)}.mock.v1`;

    const templates: Record<string,string> = {
      NOTE: JSON.stringify({
        docType: "NOTE", promptVersion: pver,
        data: { NoteAmount: 200000, InterestRate: 7.125, AmortTermMonths: 360, FirstPaymentDate: "2025-10-01", MaturityDate: "2055-10-01", LateChargePct: 5, LateChargeGraceDays: 15, BorrowerFullName: "John Q. Public" },
        evidence: {
          NoteAmount: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          InterestRate: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          AmortTermMonths: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          FirstPaymentDate: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          MaturityDate: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          LateChargePct: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          LateChargeGraceDays: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          BorrowerFullName: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) }
        }
      }),
      CD: JSON.stringify({
        docType: "CD", promptVersion: pver,
        data: { TotalLoanAmount: 200000, PAndIAmount: 1350.22, EscrowRequired: true, TaxEscrowMonthly: 250, InsuranceEscrowMonthly: 120.5, HOICarrier: "Acme Mutual", HOIPolicyNumber: "ABC-123", PropertyAddress: "123 Main St, Phoenix, AZ 85032" },
        evidence: {
          TotalLoanAmount: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          PAndIAmount: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          EscrowRequired: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          TaxEscrowMonthly: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          InsuranceEscrowMonthly: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          HOICarrier: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          HOIPolicyNumber: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          PropertyAddress: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) }
        }
      }),
      HOI: JSON.stringify({
        docType: "HOI", promptVersion: pver,
        data: { HomeownersInsCarrier: "Acme Mutual", HOIPolicyNumber: "ABC-123", HOIEffectiveDate: "2025-09-15", HOIExpirationDate: "2026-09-15", HOIPhone: null, HOIEmail: null },
        evidence: {
          HomeownersInsCarrier: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          HOIPolicyNumber: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          HOIEffectiveDate: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          HOIExpirationDate: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) }
        }
      }),
      FLOOD: JSON.stringify({
        docType: "FLOOD", promptVersion: pver,
        data: { FloodZone: "AE", FloodInsRequired: true, DeterminationIdentifier: "DET-123" },
        evidence: {
          FloodZone: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          FloodInsRequired: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) }
        }
      }),
      APPRAISAL: JSON.stringify({
        docType: "APPRAISAL", promptVersion: pver,
        data: { AppraisalDate: "2025-09-01", AppraisedValue: 450000, AppraisalFormType: "URAR" },
        evidence: {
          AppraisalDate: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) },
          AppraisedValue: { docId: "00000000-0000-0000-0000-000000000000", page: 1, textHash: "0".repeat(64) }
        }
      })
    };
    return { text: templates[type] || templates.NOTE };
  }
}

class OpenAIProvider implements LlmProvider {
  async generate({ prompt, model, temperature, maxTokens, timeoutMs }: GenArgs): Promise<LlmOutput> {
    const apiKey = process.env.OPENAI_API_KEY!;
    const mdl = model || process.env.LLM_MODEL || "gpt-4o-mini";
    const controller = new AbortController();
    const tmo = setTimeout(()=>controller.abort(), timeoutMs || Number(process.env.AI_REQUEST_TIMEOUT_MS || "60000"));

    try {
      const res = await fetch("https://api.openai.com/v1/chat/completions", {
        method: "POST",
        headers: { "Authorization": `Bearer ${apiKey}`, "Content-Type":"application/json" },
        body: JSON.stringify({
          model: mdl,
          temperature: Number(temperature ?? (process.env.AI_TEMPERATURE || "0")),
          max_tokens: Number(maxTokens ?? (process.env.AI_MAX_TOKENS || "2000")),
          messages: [
            { role: "system", content: "You are a precise data-extraction service. Output STRICT JSON, no commentary." },
            { role: "user", content: prompt }
          ]
        }),
        signal: controller.signal
      });
      if (!res.ok) throw new Error(`OpenAI error: ${res.status} ${await res.text()}`);
      const json = await res.json();
      const text = json.choices?.[0]?.message?.content?.trim() || "";
      return { text };
    } finally {
      clearTimeout(tmo);
    }
  }
}

export function getProvider(): LlmProvider {
  const p = (process.env.LLM_PROVIDER || "mock").toLowerCase();
  if (p === "openai") return new OpenAIProvider();
  return new MockProvider();
}

4) Prompt Renderer + Runner (loads .md, injects slice, validates JSON)

File: src/ai/promptRunner.ts

import fs from "fs";
import path from "path";
import { getProvider } from "./llm";
import { assertValidPromptOutput } from "../utils/validation/promptOutputValidator";

type AiItem = {
  key: string;
  value: any;
  confidence: number;
  source: "ai_doc";
  prompt_version: string;
  // evidence attachment left to worker from returned payload
};

function loadPrompt(docType: string): string {
  const p = path.resolve(process.cwd(), `prompts/${docType.toLowerCase()}.md`);
  return fs.readFileSync(p, "utf-8");
}

/** Produce a single prompt by injecting the slice into the md template. */
function renderPrompt(docType: string, slice: string): string {
  const base = loadPrompt(docType);
  return base.replace("{{DOC_TEXT_SLICE}}", slice);
}

/** Light confidence heuristic from evidence richness (bbox/snippet) + key presence. */
function computeConfidence(validJson: any): number {
  // Baseline
  let conf = 0.80;
  const ev = validJson?.evidence || {};
  const keys = Object.keys(validJson?.data || {});
  let enriched = 0;
  for (const k of keys) {
    const e = ev[k];
    if (e && e.textHash) enriched += 1;
    if (e && e.bbox) enriched += 0.5;
  }
  conf += Math.min(0.15, enriched * 0.01); // +1% per evidence, max +15%
  return Math.max(0.60, Math.min(0.95, conf));
}

/**
 * Run AI on a docType + list of text slices.
 * - We call the LLM per slice until we obtain a valid JSON that passes schema.
 * - We pick the first valid result (you can extend to multi-slice merge later).
 */
export async function runPromptPackOnSlices(docType: string, slices: {slice:string, idx:number}[]): Promise<AiItem[] | null> {
  const provider = getProvider();
  for (const s of slices) {
    const prompt = renderPrompt(docType, s.slice);
    const out = await provider.generate({
      prompt,
      model: process.env.LLM_MODEL,
      temperature: Number(process.env.AI_TEMPERATURE || "0"),
      maxTokens: Number(process.env.AI_MAX_TOKENS || "2000"),
      timeoutMs: Number(process.env.AI_REQUEST_TIMEOUT_MS || "60000")
    });

    let parsed: any = null;
    try {
      parsed = JSON.parse(out.text);
    } catch {
      continue; // try next slice
    }

    try {
      assertValidPromptOutput(parsed);
    } catch {
      continue; // try next slice
    }

    const keys = Object.keys(parsed.data || {});
    const conf = computeConfidence(parsed);
    const items: AiItem[] = keys.map(k => ({
      key: k,
      value: parsed.data[k],
      confidence: conf,
      source: "ai_doc",
      prompt_version: parsed.promptVersion || null
    }));

    // Attach evidence docId/page/hash at worker level from parsed.evidence
    // (we return the parsed object as symbol so worker can keep it)
    (items as any).__rawPromptOutput = parsed;
    return items;
  }
  return null;
}

5) Wire AI into the Extract Worker (only for missing keys)

Replace your Step-6 worker with the one below (deterministic + AI). It:

Runs deterministic per cluster.

Identifies missing keys from DOC_KEYSETS.

Slices OCR text; runs prompt; validates JSON; computes confidence.

Merges deterministic + AI candidates and calls your Authority Matrix evaluator (from Step 4).

Persists winners with lineage & versions.

File: src/workers/ExtractWorker.ts

import { mq } from "../topology";
import { deterministicExtract } from "../utils/extractors";
import { getDocTextSlices } from "../utils/ocr-slices";
import { runPromptPackOnSlices } from "../ai/promptRunner";
import { DOC_KEYSETS } from "../ai/keysets";
import { loadAuthorityMatrix } from "../utils/authority";
import { selectAuthoritativeValue } from "../authority/evaluator";
import { persistDatapoints } from "../repo";

const EXTRACTOR_VERSION = process.env.EXTRACTOR_VERSION || "v2025.09.01";
const CONF_ACCEPT       = Number(process.env.AI_ACCEPT_CONFIDENCE || "0.80");
const CONF_MIN          = Number(process.env.AI_MIN_CONFIDENCE || "0.60");

export async function startExtractWorker() {
  const matrix = loadAuthorityMatrix("config/authority-matrix.v2025-09-03.yaml", process.env.PROGRAM, process.env.INVESTOR_PROFILE);

  await mq.consume("loan.ocr.completed.q", async (msg:any, ch:any)=>{
    const { tenantId, loanId, importId, clusters, meta } = JSON.parse(msg.content.toString());
    try {
      // 1) Collect candidates
      const candidates: Record<string, any[]> = {};
      const aiEvidenceByKey: Record<string, {docId?:string,page?:number,textHash?:string}> = {};

      for (const c of clusters) {
        // deterministic first
        const det = await deterministicExtract(c.docId, c.docType);
        for (const it of det) {
          (candidates[it.key] ||= []).push({
            key: it.key, value: it.value, confidence: 1.0, source: "deterministic", docType: c.docType, extractor: "deterministic"
          });
        }

        // find missing keys for this docType
        const desired = DOC_KEYSETS[c.docType] || [];
        const presentKeys = new Set(Object.keys(candidates));
        const missing = desired.filter(k => !presentKeys.has(k));

        if (missing.length) {
          // 2) AI runner on slices (we accept first valid slice result)
          const slices = await getDocTextSlices(c.docId);
          if (slices.length) {
            const aiItems = await runPromptPackOnSlices(c.docType, slices);
            if (aiItems && aiItems.length) {
              const raw = (aiItems as any).__rawPromptOutput;
              // pull field-level evidence for lineage
              for (const ai of aiItems) {
                if (!missing.includes(ai.key)) continue; // we only use AI for missing keys
                if (ai.confidence < CONF_MIN) continue;  // discard too weak
                (candidates[ai.key] ||= []).push({
                  key: ai.key,
                  value: ai.value,
                  confidence: ai.confidence,
                  source: "ai_doc",
                  docType: c.docType,
                  extractor: "ai",
                  prompt_version: ai.prompt_version
                });

                const ev = raw?.evidence?.[ai.key];
                if (ev) aiEvidenceByKey[ai.key] = {
                  docId: ev.docId, page: ev.page, textHash: ev.textHash
                };
              }
            }
          }
        }
      }

      // 3) Select winners per key via Authority Matrix
      const finalRows: any[] = [];
      const conflictsForHITL: any[] = [];

      for (const [key, arr] of Object.entries(candidates)) {
        const res = selectAuthoritativeValue(matrix, key, arr as any);

        if (!res.winner) continue;

        const ev = aiEvidenceByKey[key] || {}; // present if AI provided evidence; deterministic lineage is attached elsewhere (OCR index)
        finalRows.push({
          key,
          value: res.winner.value,
          normalized_value: normalizeValue(key, res.winner.value),
          confidence: Math.max(0, Math.min(1, res.score ?? 1)),
          autofilled_from: res.winner.source === "ai_doc" ? "document" : res.winner.source,
          ingest_source: res.winner.source === "ai_doc" ? "document" : res.winner.source,
          evidence_doc_id: ev.docId || null,
          evidence_page: ev.page || null,
          evidence_text_hash: ev.textHash || null,
          extractor_version: EXTRACTOR_VERSION,
          prompt_version: (res.winner as any).prompt_version || null
        });

        // if close tie -> create conflict (optional: handled in Step 4 already)
        if (res.ties && res.ties.length) {
          conflictsForHITL.push({
            key,
            candidates: arr
          });
        }
      }

      // 4) Persist
      if (finalRows.length) {
        await persistDatapoints(tenantId, loanId, finalRows);
      }

      // publish next stage + ack
      await mq.publish("loan.extract","completed",{ tenantId, loanId, importId, counts:{ items: finalRows.length }, meta });
      ch.ack(msg);
    } catch (e:any) {
      // log+DLQ as per your topology
      ch.nack(msg, false, false);
    }
  });
}

function normalizeValue(key: string, v: any) {
  // hook for light normalization (strip commas for money, unify dates, etc.)
  if (v == null) return v;
  if (/Amount|Monthly|Value/i.test(key) && typeof v === "string") return Number(v.replace(/[^\d.]/g, ""));
  return v;
}


Note: We store prompt_version only for AI-sourced fields. Deterministic fields keep prompt_version=NULL, extractor_version=EXTRACTOR_VERSION.

6) Minimal AI Path Test (uses mock provider)

File: tests/ai-runner.mock.test.ts

import { runPromptPackOnSlices } from "../src/ai/promptRunner";

describe("AI runner mock", () => {
  it("returns valid NOTE JSON", async () => {
    const items = await runPromptPackOnSlices("NOTE", [{ slice: "PROMISSORY NOTE ...", idx: 0 }]);
    expect(items).toBeTruthy();
    const map = new Map(items!.map(i => [i.key, i.value]));
    expect(map.get("NoteAmount")).toBe(200000);
    expect((items as any).__rawPromptOutput).toBeTruthy();
  });
});

7) What engineering must not change

No prompting tweaks beyond the .md files already included.

No schema changesâ€”AI outputs must pass the JSON Schemas from Step 5.

No skipping Authority Matrixâ€”always run selectAuthoritativeValue.

No direct notificationsâ€”Do-Not-Ping stays enforced server-side.

8) Quick run checklist

LLM_PROVIDER=mock in .env.

Start workers; post a synthetic loan.ocr#completed message or run end-to-end.

Inspect DB loan_datapoints: AI-filled keys should have autofilled_from='document', ingest_source='document', with prompt_version set and extractor_version=EXTRACTOR_VERSION.

Verify /imports/:id mapping preview shows AI-filled fields where deterministic was missing.

Thatâ€™s Step 7 completely done: prompt rendering â†’ LLM call â†’ strict validation â†’ confidence â†’ merge â†’ lineage â†’ persist. Engineering only needs to paste this in and wire to your existing build. If/when youâ€™re ready to use a real provider, just set LLM_PROVIDER=openai and OPENAI_API_KEYâ€”no code changes required.