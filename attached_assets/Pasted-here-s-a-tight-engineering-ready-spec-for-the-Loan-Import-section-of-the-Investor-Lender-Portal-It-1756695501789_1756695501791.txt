here’s a tight, engineering-ready spec for the Loan Import section of the Investor/Lender Portal. It’s contract-first, wired to RabbitMQ, and enforces Do-Not-Ping. AI steps marked [AI].

0) Scope & Goals

Import loans via:

MISMO 3.4 XML (UCD/ULDD variants).

Bulk CSV/JSON (investor or legacy exports).

Partner API ingest (server-to-server).

Outcomes:

Map inbound → canonical keyspace with lineage.

Validate (schema + business) before persistence.

Emit events to RabbitMQ for downstream extract/QC/export.

Never contact parties if data are present in the payload or attached docs (Do-Not-Ping).

Assumptions (execute unless you say otherwise):

Max file size 200 MB per upload; streaming parser for MISMO.

Tenant isolation via Postgres RLS; uploads always tied to tenant_id.

Timezone handling: store UTC; display user TZ.

1) Data Model Additions

Tables (or columns) in schema.ts:

imports(id, tenant_id, type {mismo,csv,json,api}, filename, size, sha256, status {received,validating,errors,accepted,ingested,failed}, error_count, created_at, created_by)

import_errors(id, import_id, code, severity, pointer, message, raw_fragment JSONB, created_at)

import_mappings(id, import_id, canonical_key, source_pointer, normalized_value, evidence_hash, confidence NUMERIC, autofilled_from ENUM('document','vendor','user','payload'))

loan_candidates add source_import_id UUID NULL

loan_datapoints add ingest_source ENUM('document','vendor','user','payload') DEFAULT 'payload'

Indexes: (tenant_id,status,created_at), (import_id), GIN on import_errors.raw_fragment.

2) OpenAPI (REST) — Contract-First
openapi: 3.1.0
info: { title: LoanServe Import API, version: 1.0 }
paths:
  /imports:
    post:
      summary: Start an import (MISMO, CSV, JSON)
      requestBody:
        content:
          multipart/form-data:
            schema:
              type: object
              properties:
                type: { type: string, enum: [mismo, csv, json] }
                file: { type: string, format: binary }
                program_hint: { type: string, nullable: true } # e.g., FNMA, FRE, Portfolio
                investor_template: { type: string, nullable: true }
              required: [type, file]
      responses:
        "202": { description: accepted, headers: { Location: { schema: { type: string } } } }
  /imports/{id}:
    get:
      summary: Import status & errors
      responses:
        "200":
          content:
            application/json:
              schema:
                type: object
                properties:
                  id: { type: string, format: uuid }
                  status: { type: string, enum: [received,validating,errors,accepted,ingested,failed] }
                  error_count: { type: integer }
                  created_at: { type: string, format: date-time }
                  mapping_preview: { $ref: '#/components/schemas/MappingPreview' }
  /imports/{id}/ingest:
    post:
      summary: Commit the import and create/update a loan candidate
      responses:
        "201":
          content:
            application/json:
              schema:
                type: object
                properties:
                  loan_id: { type: string, format: uuid }
                  snapshot_hash: { type: string }
  /imports/specs/csv:
    get:
      summary: Download current CSV template & data dictionary
      responses:
        "200": { content: { text/csv: {}, application/json: {} } }

components:
  schemas:
    MappingPreview:
      type: object
      properties:
        canonical:
          type: array
          items:
            type: object
            properties:
              key: { type: string }           # e.g., InterestRate
              value: { type: string }
              normalized_value: { type: string }
              evidence: 
                type: object
                properties:
                  source_pointer: { type: string } # XPath/CSV col/JSON path
                  evidence_hash: { type: string }
              confidence: { type: number }     # [AI] for inferred fields
              autofilled_from: { type: string, enum: [payload, document, vendor, user] }


Security: OAuth2/OIDC; scopes imports.write, imports.read, loans.write.

3) RabbitMQ Topology

Exchanges (topic):

import.mismo | import.bulk | import.api

loan.extract | loan.qc | ops.monitoring

Queues:

import.validate.q (bind import.*#received)

import.map.q (bind import.*#validated)

import.commit.q (bind import.*#mapped)

import.dlq, per-stage *.retry.q with TTL

Routing keys (examples):

received, validated, mapped, errored, committed

Outbox: publish events from DB transaction to avoid loss.

4) Validation Pipeline
4.1 Schema Validation

MISMO: XSD validation on upload; streaming SAX parser to collect:

Borrower, Collateral, LoanTerms, Fees, Escrows, MI, AUS, Disclosures.

Capture XPath for each value (lineage).

CSV/JSON: Validate against our Data Dictionary (Section 7).

Required columns/paths; types; enums; date formats (YYYY-MM-DD).

4.2 Business Validation (Deterministic)

Examples: InterestRate in (0, 100), OriginalLoanAmount > 0, MaturityDate > NoteDate, State ∈ 50 states, Occupancy ∈ {Owner, Second, Investment}, TRID timing fields present if applicable, HOI required? defer to program matrix.

4.3 Normalization

Address parsing (USPS), phone/e-mail normalization, currency to cents, percentages to decimals, date coercion, enumerations to canonical codes.

4.4 Result

If any fatal errors → status errors, write rows to import_errors with pointer (XPath/column/path).

Else → status accepted and produce Mapping Preview (top 150 canonical fields with values + lineage).

5) Mapping to Canonical
5.1 Strategy

Deterministic mapping tables (code) + [AI] inference only for missing non-criticals (e.g., program code based on clues). AI inferences carry confidence and are always editable/HITL.

5.2 Examples

MISMO LOAN_DETAIL/NoteRatePercent → InterestRate

MISMO TERMS_OF_LOAN/MaturityTermMonthsCount → AmortTermMonths

CSV NoteRate → InterestRate; PPMT → P&IAmount

JSON loan.originalAmount → OriginalLoanAmount

Lineage: store source_pointer and payload SHA-256.

5.3 Do-Not-Ping Safeguard

For keys like HOI, Flood, Title, if payload contains them, set autofilled_from='payload' and suppress any outbound request paths later.

6) Ingestion & Loan Creation

Commit rules:

If loan_id provided in payload and tenant has access → update (upsert canonical fields; maintain lineage versions).

Else → create new loan_candidates with source_import_id.

Insert rows in loan_datapoints (one row per canonical key) with:

value, normalized_value, evidence_hash, confidence, ingest_source='payload', autofilled_from='payload'.

Events:

Publish loan.extract#completed (even though it’s import-based) to kick QC and Conflicts workers, or publish loan.qc#start.

7) Bulk CSV/JSON Specification (Investor-friendly)
7.1 CSV Columns (excerpt; full dictionary served by /imports/specs/csv)
LoanNumber, InvestorLoanId, LenderLoanId, BorrowerFirstName, BorrowerLastName,
PropertyStreet, PropertyCity, PropertyState, PropertyZip,
OriginalLoanAmount, InterestRate, RateType, PaymentType, AmortTermMonths,
FirstPaymentDate, MaturityDate, PnIAmount,
EscrowRequired, TaxEscrowMonthly, InsuranceEscrowMonthly,
HOICarrier, HOIPolicyNumber, HOIPhone, HOIEmail, HOIEffectiveDate, HOIExpirationDate,
FloodZone, FloodInsRequired, TitleCompanyName, TitleFileNumber,
AppraisedValue, AppraisalDate, OccupancyType, LoanPurpose, LTV, CLTV


Rules:

Dates: YYYY-MM-DD

Currency: decimal (2 places)

Percentages: decimal (e.g., 7.125)

State: 2-letter USPS

Booleans: true|false|1|0

7.2 JSON Schema (excerpt)
{
  "type":"object",
  "properties":{
    "loanNumber":{"type":"string"},
    "borrowers":{"type":"array","items":{"type":"object","properties":{
      "firstName":{"type":"string"},"lastName":{"type":"string"},"email":{"type":"string","format":"email"}
    }}},
    "collateral":{"type":"object","properties":{
      "address":{"type":"object","properties":{
        "street":{"type":"string"},"city":{"type":"string"},
        "state":{"type":"string","pattern":"^[A-Z]{2}$"},"zip":{"type":"string","pattern":"^\\d{5}(-\\d{4})?$"}
      }}
    }},
    "loanTerms":{"type":"object","properties":{
      "originalAmount":{"type":"number"},"interestRate":{"type":"number"},
      "rateType":{"enum":["Fixed","ARM"]},"amortTermMonths":{"type":"integer"},
      "firstPaymentDate":{"type":"string","format":"date"},
      "maturityDate":{"type":"string","format":"date"}
    }},
    "insurance":{"type":"object","properties":{
      "carrier":{"type":"string"},"policyNumber":{"type":"string"},
      "effectiveDate":{"type":"string","format":"date"},"expirationDate":{"type":"string","format":"date"}
    }}
  },
  "required":["loanTerms","collateral","borrowers"]
}

8) Service/Worker Stubs (TypeScript)

routes.ts — POST /imports

router.post("/imports", auth("imports.write"), upload.single("file"), async (req, res) => {
  const { type, program_hint, investor_template } = req.body;
  const fileMeta = await storage.save(req.file); // storage.ts
  const imp = await repo.createImport({ tenantId: req.tenant.id, type, ...fileMeta, status: "received" });
  await mq.publish("import." + type, "received", { importId: imp.id, tenantId: req.tenant.id });
  res.status(202).location(`/imports/${imp.id}`).end();
});


service.ts — validator worker

mq.consume("import.validate.q", async (msg) => {
  const { importId, tenantId, type } = JSON.parse(msg.content.toString());
  const imp = await repo.getImport(importId);
  try {
    if (type === "mismo") await validateMismo(imp); else await validateCsvJson(imp);
    await mq.publish(`import.${type}`, "validated", { importId, tenantId });
  } catch (e) {
    await repo.recordImportErrors(importId, e.errors);
    await mq.publish(`import.${type}`, "errored", { importId, tenantId, count: e.errors.length });
  } finally { mq.ack(msg); }
});


service.ts — mapper worker

mq.consume("import.map.q", async (msg) => {
  const { importId, tenantId, type } = JSON.parse(msg.content.toString());
  const mapping = await mapToCanonical(type, importId); // returns MappingPreview
  await repo.saveMappingPreview(importId, mapping);
  await mq.publish(`import.${type}`, "mapped", { importId, tenantId });
  mq.ack(msg);
});


routes.ts — POST /imports/{id}/ingest

router.post("/imports/:id/ingest", auth("loans.write"), async (req, res) => {
  const imp = await repo.getImport(req.params.id);
  if (imp.status !== "accepted" && imp.status !== "mapped") return res.status(409).json({ error: "NOT_ACCEPTED" });
  const { loanId, snapshotHash } = await repo.commitImportToLoan(imp); // writes loan_datapoints with ingest_source='payload'
  await mq.publish("loan.qc", "start", { loanId, sourceImportId: imp.id });
  res.status(201).json({ loan_id: loanId, snapshot_hash: snapshotHash });
});

9) Do-Not-Ping Enforcement (import path)

When a payload provides HOI / Flood / Title / Appraisal data, persist with autofilled_from='payload', ingest_source='payload'.

Downstream notification-service guard checks for any canonical value with adequate confidence from payload|document|vendor; if present, block notifications with 409 DoNotPingPolicy and include evidence pointer (e.g., source_pointer: '/MISMO/.../PolicyIdentifier').

10) Monitoring & Alerting

Metrics (Prometheus):

import_received_total{type}

import_validated_total{type}

import_errors_total{type,code}

import_map_duration_seconds (histogram)

import_commit_duration_seconds

dlq_messages_total{queue}

Alerts:

Error rate > 5% over 15m → page Ops.

DLQ depth > 100 → throttle uploader (429 from /imports).

11) Error Codes (canonical)

MISMO_XSD_INVALID (fatal)

CSV_MISSING_COLUMN (fatal)

TYPE_COERCION_ERROR (fatal)

ENUM_INVALID (fatal)

DATE_SEQUENCE_INVALID (fatal)

REQUIRED_FIELD_MISSING (fatal)

NORMALIZATION_WARNING (warn)

PROGRAM_MISMATCH (warn)

Each error stores pointer (XPath/column/path) and raw_fragment.

12) Acceptance Tests (high-value)

MISMO-OK-01
Given a valid MISMO 3.4 file
When POST /imports (mismo)
Then import status becomes accepted, MappingPreview returns top keys with XPaths and normalized values.

CSV-ERR-02
Given CSV missing InterestRate column
When POST /imports (csv)
Then status errors, import_errors contains CSV_MISSING_COLUMN with pointer InterestRate.

INGEST-UPSERT-03
Given an accepted import referencing an existing loan_id
When POST /imports/{id}/ingest
Then upsert datapoints with lineage preserved and ingest_source='payload'.

DNP-HOI-04
Given payload includes HOI carrier/policy/dates
When downstream /notifications tries to request HOI
Then API returns 409 DoNotPingPolicy with evidence pointer to payload.

RMQ-DLQ-05
Given validator throws 3 times
Then message routes to import.dlq and alert fires.

MAPPING-LINEAGE-06
Given MISMO mapping
When viewing Data tab
Then each field shows source_pointer (XPath) and evidence_hash.

13) Build Order (vertical slice)

Implement /imports + storage + RabbitMQ received.

MISMO XSD validation + SAX streaming parser + lineage capture.

CSV/JSON validator with data dictionary endpoint /imports/specs/csv.

Mapping to canonical + preview.

Commit to loan + emit loan.qc#start.

Do-Not-Ping enforcement flags on imported fields.

Metrics + DLQ + alerts.

Acceptance tests above.

14) Developer Notes

Streaming parsing is mandatory for MISMO to avoid memory spikes.

Normalize enums at the edge (validator) to keep mapping deterministic.

Keep mapping tables versioned; store mapping_version in imports.

Always write lineage (pointer + hash) so explainability is first-class.

If you want, I’ll generate:

The CSV template + JSON data dictionary file,

The MISMO XPath → canonical mapping table (YAML),

And a ready-to-run validator worker scaffold you can drop into service.ts.

You said:
Can postgres store objects 
ChatGPT said:
Thought for 9s

Short answer: yes—PostgreSQL can store “objects,” several ways. The best fit depends on how dynamic the shape is and how you’ll query it.

Common options

jsonb (recommended for dynamic objects)

Schemaless-ish, binary-stored JSON with rich operators and indexes.

Great when fields vary across records (e.g., vendor payloads, MISMO fragments, evidence).

Query & index paths you care about.

-- Table with a jsonb "object"
CREATE TABLE loan_payloads (
  id            uuid PRIMARY KEY,
  loan_id       uuid NOT NULL,
  payload       jsonb NOT NULL,
  created_at    timestamptz DEFAULT now()
);

-- GIN index for containment/keys (@>, ?, ?& , ?|)
CREATE INDEX loan_payloads_payload_gin ON loan_payloads USING GIN (payload);

-- Pull a field and index it (generated column)
ALTER TABLE loan_payloads
ADD COLUMN interest_rate numeric GENERATED ALWAYS AS
  ((payload->'loanTerms'->>'interestRate')::numeric) STORED;

CREATE INDEX loan_payloads_ir_idx ON loan_payloads (interest_rate);

-- Examples
SELECT payload->'insurance'->>'policyNumber' AS pol
FROM loan_payloads
WHERE payload @> '{"insurance":{"carrier":"Acme Mutual"}}';


Composite types (for strongly-typed “object” records)

Define a structured type once; use it as a column or array.

Great when the shape is stable (e.g., Address, Name).

CREATE TYPE address AS (
  street text,
  city   text,
  state  char(2),
  zip    text
);

CREATE TABLE properties (
  id       uuid PRIMARY KEY,
  address  address NOT NULL
);

-- Access fields
SELECT (address).city FROM properties;


Arrays (of scalars or composite types)

Use when the object contains lists and you don’t need heavy relational joins.

CREATE TABLE appraisal_docs(
  id uuid PRIMARY KEY,
  pages int[] NOT NULL
);


bytea or Large Objects (for binary blobs)

For files/binaries; bytea is simplest; Large Object API for very big streams.

Use object storage (S3/GCS) for large PDFs and keep only URIs + hashes in Postgres.

CREATE TABLE doc_blobs(
  id uuid PRIMARY KEY,
  loan_id uuid,
  content bytea,            -- ok for small things like thumbnails/signatures
  sha256  text NOT NULL
);

Querying & indexing JSONB effectively

Operators: -> (json), ->> (text), #>> (path), @> (containment), ? (key exists).

Indexes:

GIN on the whole jsonb for containment/existence.

Expression indexes on specific paths you filter/sort by.

Generated columns + btree indexes for type-safe, fast filtering/sorting.

Validation: add CHECK constraints on extracted paths:

ALTER TABLE loan_payloads
ADD CONSTRAINT chk_rate_range
CHECK ((payload->'loanTerms'->>'interestRate')::numeric BETWEEN 0 AND 100);

When to use which (pragmatic guidance)

Critical, frequently-queried fields (e.g., InterestRate, NoteAmount, LTV):
Put them as first-class columns (with types, FKs, constraints). You can still store the full object in jsonb for lineage.

Variable / vendor-specific / rarely queried bits (e.g., raw MISMO fragments, evidence boxes, vendor responses):
Store as jsonb with GIN + a few generated columns for the handful of paths you actually query.

Stable nested shapes you reuse everywhere (e.g., Address):
Consider a composite type or a separate normalized table with FK (often better for joins and constraints).

Large PDFs/images:
Prefer object storage; keep URIs + hashes in Postgres.

Gotchas

jsonb is fast for keys you index, but don’t expect great performance on arbitrary deep ad-hoc paths—add expression or generated-column indexes.

Big jsonb updates can bloat due to MVCC/TOAST—batch updates and keep objects reasonably sized.

Composite types are rigid: changing the type requires ALTER TYPE and can be painful; use them when the shape is truly stable.

LoanServe recommendation (fits what we’ve been building)

Canonical facts: typed columns in tables like loan_datapoints (plus evidence pointers).

Evidence & payloads (MISMO slices, Textract blocks, lineage): store in jsonb columns with GIN, plus generated columns for the 10–20 fields you filter on (program code, HOI carrier, policy number, etc.).

Addresses/Names: either composite types or normalized child tables if you need FKs/constraints.

Docs: store in S3; Postgres keeps the URI, hash, doc_type, confidence.

If you want, I can sketch the exact DDL (columns, generated fields, indexes) for your loan_datapoints + evidence tables so engineering can drop it in.

You said:
You forgot pdf upload 
ChatGPT said:
Thought for 22s

You’re right—I omitted PDF upload in the Import spec. Here’s the PDF Import addendum wired to RabbitMQ and the Do-Not-Ping guard, fully consistent with the rest of the plan.

PDF Import — Engineering Spec (Investor/Lender Portal)
0) Scope

Allow investors/lenders to import single PDF or ZIP of PDFs as a loan source. Run the DocChunker → OCR → Extraction [AI] pipeline, map to canonical keyspace with evidence, and never contact any party if the extracted documents already contain the data.

1) Data Model (diffs)

imports

type ENUM: add 'pdf' and 'zip'

docset_id UUID NULL — links to a grouped document set created during split

progress JSONB — stage status: {split:%, ocr:%, extract:%}

import_mappings

Add evidence shape for PDFs:

evidence JSONB: { doc_id, page, text_hash } (instead of XPath)

loan_datapoints

ingest_source DEFAULT 'payload' → also allow 'document'

When filled from PDFs, set ingest_source='document', autofilled_from='document'.

2) OpenAPI (additions)
paths:
  /imports:
    post:
      summary: Start an import (MISMO, CSV, JSON, PDF/ZIP)
      requestBody:
        content:
          multipart/form-data:
            schema:
              type: object
              properties:
                type: { type: string, enum: [mismo, csv, json, pdf, zip] }
                file: { type: string, format: binary }
                program_hint: { type: string, nullable: true }
              required: [type, file]
      responses:
        "202": { description: accepted, headers: { Location: { schema: { type: string } } } }

  /imports/{id}:
    get:
      summary: Import status, progress, and mapping preview (if available)
      responses:
        "200":
          content:
            application/json:
              schema:
                type: object
                properties:
                  id: { type: string, format: uuid }
                  status: { type: string, enum: [received,validating,errors,accepted,ingested,failed] }
                  progress:
                    type: object
                    properties:
                      split: { type: number }     # %
                      ocr: { type: number }       # %
                      extract: { type: number }   # %
                  mapping_preview: { $ref: '#/components/schemas/MappingPreview' }


UI notes

Import tab now includes PDF/ZIP. Show live Split/Classify preview [AI] and a progress bar (split/ocr/extract). Allow page surgery/reclass before extraction finishes.

3) RabbitMQ Topology (bridge to doc pipeline)

New exchange: import.pdf (topic)
Routing keys: received, validated, split, ocr, extracted, errored, mapped, committed
Queues:

import.pdf.validate.q (MIME/virus/password checks)

import.pdf.bridge_docs.q (publishes into doc pipeline)

import.pdf.track_progress.q (subscribes to doc events to update imports.progress)

import.pdf.commit.q

Bridge:

After received → validate → publish loan.docs#uploaded with {loanCandidateId?, importId, fileUri}.

Subscribe to doc pipeline events (loan.docs#chunked, loan.ocr#completed, loan.extract#completed) to update imports.progress and to assemble a MappingPreview from loan_datapoints.

4) Processing Stages

Validate

Virus scan, MIME sniff (application/pdf, application/zip), size/page limits (default: ≤ 500MB, ≤ 3,000 pages).

If ZIP: enumerate PDFs, store, emit one loan.docs#uploaded per file; group under docset_id.

Split/Classify [AI] (DocChunker)

Page segmentation → cluster pages → assign doc_type + confidence.

Allow UI page surgery; re-emit loan.docs#chunked after edits.

OCR [AI]

Textract on each cluster; store word boxes and reflowed text.

Extraction [AI]

Prompt packs per doc-type (Note, Deed, CD/LE, Title, HOI, Flood, Appraisal, HOA, Disclosures).

Merge & vote across docs; compute field confidence.

Write loan_datapoints with ingest_source='document', autofilled_from='document', plus {doc_id,page,text_hash} evidence.

Mapping Preview

Build top-150 canonical keys + normalized values + lineage for /imports/{id}.

Status → accepted when minimum critical set is present (e.g., LoanNumber, BorrowerName, SubjectAddress, NoteAmount, InterestRate, Term, FirstPaymentDate, MaturityDate).

Ingest

POST /imports/{id}/ingest creates/updates loan_candidates.

Publish loan.qc#start (and/or conflicts.detected if any), Do-Not-Ping auto-suppress later notifications where evidence exists.

5) Do-Not-Ping Guard (PDF path)

Before any notification, notification-service checks for canonical values from payload|document|vendor with confidence ≥ threshold.

For HOI, Flood, Title, Appraisal present in extracted datapoints → suppress borrower/escrow contact. Store no_contact_reason='evidence_present'.

6) Error Codes (PDF-specific)

PDF_PASSWORD_REQUIRED (fatal; include friendly prompt to re-upload with password)

PDF_CORRUPT (fatal)

PDF_TOO_LARGE / PDF_TOO_MANY_PAGES (fatal; suggest ZIP split)

DOC_SPLIT_FAILED (fatal; allow manual page surgery then retry)

OCR_TIMEOUT (retry; then DLQ)

UNSUPPORTED_PDF_FEATURE (warn; rasterize fallback)

7) Acceptance Tests

PDF-OK-01
Given a valid closing package PDF (Note, Deed, CD, HOI)
When POST /imports with type=pdf
Then imports.status progresses to accepted, progress shows split/ocr/extract > 0, and mapping_preview includes HOI fields with evidence {doc_id,page}.

PDF-DNP-HOI-02
Given extracted HOI carrier/policy/dates from the PDF
When any service calls POST /notifications to request HOI
Then return 409 DoNotPingPolicy with evidence pointer (doc_id, page, hash).

PDF-PASSWD-03
Given a password-protected PDF
When POST /imports
Then status errors with PDF_PASSWORD_REQUIRED.

PDF-EDIT-04
Given misclassified pages
When user fixes via page surgery
Then re-run DocChunker; extraction uses updated doc types; mapping preview updates.

PDF-DLQ-05
Given OCR worker fails thrice
Then message ends in import.pdf.dlq and Monitoring displays alert.

8) Routes & Workers (stubs)

routes.ts — start PDF import

router.post("/imports", auth("imports.write"), upload.single("file"), async (req, res) => {
  const { type } = req.body; // includes 'pdf' or 'zip'
  const meta = await storage.save(req.file); // storage.ts
  const imp = await repo.createImport({ tenantId: req.tenant.id, type, ...meta, status: "received" });
  await mq.publish("import.pdf", "received", { importId: imp.id, tenantId: req.tenant.id, fileUri: meta.uri });
  res.status(202).location(`/imports/${imp.id}`).end();
});


service.ts — bridge to doc pipeline

mq.consume("import.pdf.bridge_docs.q", async (msg) => {
  const { importId, tenantId, fileUri } = JSON.parse(msg.content.toString());
  // create a loan candidate shell to anchor doc pipeline (optional)
  const loanId = await repo.ensureLoanCandidate({ tenantId, sourceImportId: importId });
  await mq.publish("loan.docs", "uploaded", { loanId, importId, fileUri });
  mq.ack(msg);
});


service.ts — track progress from doc events

mq.consume("loan.docs.events", async (msg) => {
  const evt = JSON.parse(msg.content.toString());
  const { importId, stage } = evt.meta || {};
  if (importId) await repo.updateImportProgress(importId, stage); // e.g., {split:40}
  mq.ack(msg);
});

9) UI Additions (Import tab)

Source selector: MISMO • CSV/JSON • PDF/ZIP

Upload area: drag&drop, shows file list, SHA-256, size.

Progress widget: Split • OCR • Extract with percentages.

Live Preview [AI]: Doc list with confidence chips; page surgery (drag to doc type); side-by-side source + extracted fields with evidence highlight.

Commit button: Enabled when status accepted; shows what will be ingested; warns if any fatal defects exist.

10) Operational Limits & Defaults

Max file: 500MB, Max pages per file: 3,000 (configurable).

Concurrency caps per tenant; backpressure when DLQ > threshold.

Password-protected PDFs: reject with actionable error; allow re-upload w/ password parameter in the future.

TL;DR

PDF/ZIP is now a first-class Import type.

It uses the same DocChunker → OCR → Extraction [AI] path as escrow uploads, but is driven from /imports so investors can onboard loans with only documents.

All extracted values are marked autofilled_from='document', powering the Do-Not-Ping guard.

RabbitMQ integration updates: import.pdf exchange + bridge into loan.docs events, with progress tracking and robust DLQs.

Want me to generate the SQL migrations for these new columns/tables and a worker scaffold that updates imports.progress from doc events?

You said:
Continue 
ChatGPT said:
Thought for 24s

Absolutely—here’s the continuation with concrete artifacts you can drop into your repo: SQL migrations, RabbitMQ topology for PDF imports, worker scaffolds, API additions, Do-Not-Ping guard, and metrics. (AI steps marked [AI].)

1) Postgres — SQL migrations (001_imports_pdf.sql)
-- extensions (UUIDs)
CREATE EXTENSION IF NOT EXISTS pgcrypto;

-- 1) imports table
CREATE TABLE IF NOT EXISTS imports (
  id              uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  tenant_id       uuid NOT NULL,
  type            text NOT NULL CHECK (type IN ('mismo','csv','json','pdf','zip')),
  filename        text NOT NULL,
  size_bytes      bigint NOT NULL CHECK (size_bytes >= 0),
  sha256          text NOT NULL,
  docset_id       uuid NULL,
  status          text NOT NULL CHECK (status IN ('received','validating','errors','accepted','ingested','failed')),
  error_count     integer NOT NULL DEFAULT 0 CHECK (error_count >= 0),
  progress        jsonb NOT NULL DEFAULT '{}'::jsonb,    -- e.g. {"split":40,"ocr":65,"extract":20}
  mapping_version text NULL,
  created_by      uuid NOT NULL,
  created_at      timestamptz NOT NULL DEFAULT now(),
  updated_at      timestamptz NOT NULL DEFAULT now()
);

CREATE INDEX IF NOT EXISTS idx_imports_tenant_status_created
  ON imports (tenant_id, status, created_at DESC);

-- 2) import_errors
CREATE TABLE IF NOT EXISTS import_errors (
  id           uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  import_id    uuid NOT NULL REFERENCES imports(id) ON DELETE CASCADE,
  code         text NOT NULL,
  severity     text NOT NULL CHECK (severity IN ('fatal','warn')),
  pointer      text NOT NULL,                      -- XPath / CSV column / JSON path / "pdf:docId:page"
  message      text NOT NULL,
  raw_fragment jsonb NULL,
  created_at   timestamptz NOT NULL DEFAULT now()
);
CREATE INDEX IF NOT EXISTS idx_import_errors_import ON import_errors(import_id);
CREATE INDEX IF NOT EXISTS idx_import_errors_rawfrag ON import_errors USING GIN (raw_fragment);

-- 3) import_mappings
CREATE TABLE IF NOT EXISTS import_mappings (
  id              uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  import_id       uuid NOT NULL REFERENCES imports(id) ON DELETE CASCADE,
  canonical_key   text NOT NULL,
  normalized_value text NULL,
  source_pointer  text NULL,                       -- XPath/CSVcol/JSONpath or "pdf:docId:page"
  evidence_hash   text NULL,                       -- sha256 of source fragment
  confidence      numeric(5,4) NULL,               -- [AI]
  autofilled_from text NOT NULL DEFAULT 'payload' CHECK (autofilled_from IN ('payload','document','vendor','user')),
  created_at      timestamptz NOT NULL DEFAULT now()
);
CREATE INDEX IF NOT EXISTS idx_import_mappings_import ON import_mappings(import_id);
CREATE INDEX IF NOT EXISTS idx_import_mappings_key ON import_mappings(canonical_key);

-- 4) loan_candidates link to import
ALTER TABLE loan_candidates
  ADD COLUMN IF NOT EXISTS source_import_id uuid NULL REFERENCES imports(id);

-- 5) loan_datapoints source flags
DO $$
BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM information_schema.columns
    WHERE table_name='loan_datapoints' AND column_name='ingest_source'
  ) THEN
    ALTER TABLE loan_datapoints
      ADD COLUMN ingest_source text NOT NULL DEFAULT 'payload' CHECK (ingest_source IN ('payload','document','vendor','user'));
  END IF;

  IF NOT EXISTS (
    SELECT 1 FROM information_schema.columns
    WHERE table_name='loan_datapoints' AND column_name='autofilled_from'
  ) THEN
    ALTER TABLE loan_datapoints
      ADD COLUMN autofilled_from text NOT NULL DEFAULT 'payload' CHECK (autofilled_from IN ('payload','document','vendor','user'));
  END IF;

  -- evidence fields (doc lineage for PDFs)
  IF NOT EXISTS (SELECT 1 FROM information_schema.columns
                 WHERE table_name='loan_datapoints' AND column_name='evidence_doc_id') THEN
    ALTER TABLE loan_datapoints ADD COLUMN evidence_doc_id uuid NULL;
  END IF;
  IF NOT EXISTS (SELECT 1 FROM information_schema.columns
                 WHERE table_name='loan_datapoints' AND column_name='evidence_page') THEN
    ALTER TABLE loan_datapoints ADD COLUMN evidence_page integer NULL CHECK (evidence_page >= 0);
  END IF;
  IF NOT EXISTS (SELECT 1 FROM information_schema.columns
                 WHERE table_name='loan_datapoints' AND column_name='evidence_text_hash') THEN
    ALTER TABLE loan_datapoints ADD COLUMN evidence_text_hash text NULL;
  END IF;
END $$;

-- 6) RLS skeleton (adapt to your policy functions)
ALTER TABLE imports ENABLE ROW LEVEL SECURITY;
ALTER TABLE import_errors ENABLE ROW LEVEL SECURITY;
ALTER TABLE import_mappings ENABLE ROW LEVEL SECURITY;

-- Example tenant policy (replace with your tenant resolver)
CREATE POLICY p_imports_tenant ON imports
  USING (tenant_id::text = current_setting('app.tenant_id', true));

CREATE POLICY p_import_errors_tenant ON import_errors
  USING (import_id IN (SELECT id FROM imports WHERE tenant_id::text = current_setting('app.tenant_id', true)));

CREATE POLICY p_import_mappings_tenant ON import_mappings
  USING (import_id IN (SELECT id FROM imports WHERE tenant_id::text = current_setting('app.tenant_id', true)));

2) RabbitMQ topology — init-queues.ts (additions)
// init-queues.ts (add to your existing bootstrap)
import { Channel } from "amqplib";

export async function declareImportPdf(channel: Channel) {
  // Exchanges
  await channel.assertExchange("import.pdf", "topic", { durable: true });
  await channel.assertExchange("loan.docs", "topic", { durable: true });     // assumed existing
  await channel.assertExchange("ops.monitoring", "topic", { durable: true });

  // DLX
  await channel.assertExchange("dlx", "topic", { durable: true });

  // Helper to make a queue with DLQ + retry
  const q = async (name: string, rk: string) => {
    await channel.assertQueue(name, {
      durable: true,
      arguments: {
        "x-dead-letter-exchange": "dlx",
        "x-dead-letter-routing-key": `${name}.dlq`,
        "x-message-ttl": 0
      }
    });
    await channel.bindQueue(name, "import.pdf", rk);

    // retry
    await channel.assertQueue(`${name}.retry`, {
      durable: true,
      arguments: {
        "x-dead-letter-exchange": "import.pdf",
        "x-dead-letter-routing-key": rk,
        "x-message-ttl": 15000
      }
    });

    // dlq
    await channel.assertQueue(`${name}.dlq`, { durable: true });
    await channel.bindQueue(`${name}.dlq`, "dlx", `${name}.dlq`);
  };

  await q("import.pdf.validate.q", "received");
  await q("import.pdf.bridge_docs.q", "validated");
  await q("import.pdf.track_progress.q", "mapped");      // listens for mapped → commit
  await q("import.pdf.commit.q", "mapped");

  // Bind to doc pipeline events so we can track progress (expects meta.importId on messages)
  await channel.assertQueue("import.pdf.doc-events.q", { durable: true });
  await channel.bindQueue("import.pdf.doc-events.q", "loan.docs", "chunked");
  await channel.bindQueue("import.pdf.doc-events.q", "loan.docs", "ocr.completed");
  await channel.bindQueue("import.pdf.doc-events.q", "loan.docs", "extract.completed");
}

3) Routes — add PDF to /imports (excerpt for routes.ts)
// routes.ts
router.post("/imports", auth("imports.write"), upload.single("file"), async (req, res) => {
  const { type } = req.body; // 'mismo' | 'csv' | 'json' | 'pdf' | 'zip'
  if (!['mismo','csv','json','pdf','zip'].includes(type)) return res.status(400).json({ error: "TYPE_INVALID" });

  const meta = await storage.save(req.file); // storage.ts -> returns { uri, mime, size, sha256, filename }
  const imp = await repo.createImport({
    tenantId: req.tenant.id,
    type,
    filename: meta.filename,
    sizeBytes: meta.size,
    sha256: meta.sha256,
    status: 'received',
    createdBy: req.user.id
  });

  // publish to proper exchange
  const ex = type === 'pdf' || type === 'zip' ? 'import.pdf' : `import.${type}`;
  await mq.publish(ex, "received", { importId: imp.id, tenantId: req.tenant.id, fileUri: meta.uri, mime: meta.mime });

  res.status(202).location(`/imports/${imp.id}`).end();
});

router.get("/imports/:id", auth("imports.read"), async (req, res) => {
  const data = await repo.getImportWithPreview(req.params.id, req.tenant.id);
  if (!data) return res.status(404).end();
  res.json(data);
});

router.post("/imports/:id/ingest", auth("loans.write"), async (req, res) => {
  const result = await repo.commitImportToLoan(req.params.id, req.tenant.id);
  if (!result) return res.status(409).json({ error: "NOT_ACCEPTED" });
  await mq.publish("loan.qc", "start", { loanId: result.loanId, sourceImportId: req.params.id });
  res.status(201).json({ loan_id: result.loanId, snapshot_hash: result.snapshotHash });
});

4) Workers — service.ts scaffolds
// service.ts
import { mq } from "./topology";
import { repo } from "./repo";
import { audit } from "./auditService";
import { classifyMime, scanForViruses } from "./crm-utils"; // or wherever your utils live

export async function startImportPdfWorkers() {
  // 1) validate
  await mq.consume("import.pdf.validate.q", async (msg, ch) => {
    const payload = JSON.parse(msg.content.toString());
    try {
      const { importId, fileUri, mime } = payload;
      const ok = await scanForViruses(fileUri);
      if (!ok) throw { code: "PDF_CORRUPT", pointer: "file", msg: "Virus or corruption detected", fatal: true };
      const t = classifyMime(mime);
      if (t !== 'pdf' && t !== 'zip') throw { code: "TYPE_INVALID", pointer: "mime", msg: `Unsupported mime ${mime}`, fatal: true };

      await repo.updateImportStatus(importId, "validating");
      await mq.publish("import.pdf", "validated", payload);
      ch.ack(msg);
    } catch (e: any) {
      await repo.recordImportErrors(payload.importId, [{
        code: e.code || "PDF_VALIDATION_ERROR",
        severity: e.fatal ? "fatal" : "warn",
        pointer: e.pointer || "file",
        message: e.msg || "Validation failed",
        raw_fragment: null
      }]);
      await repo.updateImportStatus(payload.importId, "errors");
      ch.ack(msg);
    }
  });

  // 2) bridge → doc pipeline
  await mq.consume("import.pdf.bridge_docs.q", async (msg, ch) => {
    const { importId, tenantId, fileUri } = JSON.parse(msg.content.toString());
    const loanId = await repo.ensureLoanCandidate({ tenantId, sourceImportId: importId });
    await mq.publish("loan.docs", "uploaded", { loanId, importId, fileUri, meta: { importId } });
    await repo.updateImportStatus(importId, "received"); // stays until mapping ready
    await audit("import.pdf.bridged", loanId, { importId, fileUri });
    ch.ack(msg);
  });

  // 3) track progress from doc events
  await mq.consume("import.pdf.doc-events.q", async (msg, ch) => {
    const evt = JSON.parse(msg.content.toString());
    const { importId } = (evt.meta || {});
    if (importId) {
      const stage = evt.type || evt.routingKey; // 'chunked' | 'ocr.completed' | 'extract.completed'
      await repo.bumpImportProgress(importId, stage); // implement % math inside repo
    }
    ch.ack(msg);
  });

  // 4) commit when mapped/accepted (you may trigger this from extract.completed → mapper)
  await mq.consume("import.pdf.commit.q", async (msg, ch) => {
    const { importId, tenantId } = JSON.parse(msg.content.toString());
    const imp = await repo.getImport(importId);
    if (!imp || (imp.status !== "accepted" && imp.status !== "mapped")) return ch.ack(msg);
    const { loanId, snapshotHash } = await repo.commitImportToLoan(importId, tenantId);
    await mq.publish("loan.qc", "start", { loanId, sourceImportId: importId });
    await repo.updateImportStatus(importId, "ingested");
    ch.ack(msg);
  });
}

5) Do-Not-Ping guard — notification-service.ts (server-side enforcement)
// notification-service.ts
export async function canSatisfyFromDocsOrVendors(loanId: string, keys: string[], minConf = 0.80) {
  // 1) check canonical values from payload/document/vendor
  const vals = await repo.getDatapoints(loanId, keys);
  const ok = keys.every(k => {
    const v = vals[k];
    if (!v) return false;
    const sourceOk = ['payload','document','vendor'].includes(v.autofilled_from);
    const confOk = (v.confidence ?? 1) >= minConf;
    return sourceOk && confOk && !!v.value;
  });
  if (ok) return { satisfied: true, via: 'canonical', evidence: vals };

  // 2) optional vendor backfills here (e.g., SSR, flood); if obtained, persist as ingest_source='vendor' then return satisfied=true

  return { satisfied: false };
}

export async function sendNotificationGuarded(input: {
  template_code: string,
  loan_id: string,
  to_party: string,
  params: Record<string, any>
}) {
  // derive required keys by template
  const requiredKeysByTemplate: Record<string, string[]> = {
    BORR_HOI_REQUEST: ['HomeownersInsCarrier','HOIPolicyNumber','HOIEffectiveDate','HOIExpirationDate'],
    ESC_FLOOD_REQUEST: ['FloodZone','FloodInsRequired']
  };
  const keys = requiredKeysByTemplate[input.template_code] || [];
  if (keys.length) {
    const dnp = await canSatisfyFromDocsOrVendors(input.loan_id, keys);
    if (dnp.satisfied) {
      // log suppression
      await repo.recordNotificationSuppressed(input.loan_id, input.template_code, 'evidence_present', dnp.evidence);
      const err: any = new Error("DoNotPingPolicy");
      err.code = "DoNotPingPolicy";
      err.evidence = dnp.evidence;
      throw err;
    }
  }
  // proceed to actually send
  return sendNotificationRaw(input);
}


Client-facing API still returns 409 DoNotPingPolicy when this error bubbles.

6) Mapping preview builder (for /imports/{id})
// repo.ts (sketch)
export async function buildMappingPreview(importId: string) {
  // Gather top canonical keys from loan_datapoints linked via source_import_id or docset
  const rows = await db.any(`
    SELECT key, value, normalized_value, confidence, autofilled_from,
           evidence_doc_id, evidence_page, evidence_text_hash
    FROM loan_datapoints
    WHERE loan_id = (SELECT source_import_id FROM loan_candidates WHERE source_import_id = $1 LIMIT 1)
    ORDER BY key ASC
    LIMIT 150
  `, [importId]);

  return {
    canonical: rows.map(r => ({
      key: r.key,
      value: r.value,
      normalized_value: r.normalized_value,
      confidence: r.confidence,
      autofilled_from: r.autofilled_from,
      evidence: r.evidence_doc_id ? {
        source_pointer: `pdf:${r.evidence_doc_id}:${r.evidence_page}`,
        evidence_hash: r.evidence_text_hash
      } : null
    }))
  };
}

7) Prometheus metrics (emit from workers)
// monitoring.ts
import client from "prom-client";
export const importReceived = new client.Counter({ name: "import_received_total", help: "imports received", labelNames: ["type"] });
export const importValidated = new client.Counter({ name: "import_validated_total", help: "imports validated", labelNames: ["type"] });
export const importErrors = new client.Counter({ name: "import_errors_total", help: "import errors", labelNames: ["type","code"] });
export const importMapDuration = new client.Histogram({ name: "import_map_duration_seconds", help: "mapping time", buckets: [0.1,0.5,1,2,5,10] });
export const dlqMessages = new client.Gauge({ name: "dlq_messages_total", help: "DLQ depth", labelNames: ["queue"] });

// usage example in worker:
importReceived.inc({ type: "pdf" });


Expose /metrics in routes.ts for Prometheus scraping (auth-gated).

8) Error codes (server constants)
export const ImportErrors = {
  PDF_PASSWORD_REQUIRED: "PDF_PASSWORD_REQUIRED",
  PDF_CORRUPT: "PDF_CORRUPT",
  PDF_TOO_LARGE: "PDF_TOO_LARGE",
  PDF_TOO_MANY_PAGES: "PDF_TOO_MANY_PAGES",
  DOC_SPLIT_FAILED: "DOC_SPLIT_FAILED",
  OCR_TIMEOUT: "OCR_TIMEOUT",
  UNSUPPORTED_PDF_FEATURE: "UNSUPPORTED_PDF_FEATURE",
  CSV_MISSING_COLUMN: "CSV_MISSING_COLUMN",
  MISMO_XSD_INVALID: "MISMO_XSD_INVALID",
  TYPE_COERCION_ERROR: "TYPE_COERCION_ERROR",
  ENUM_INVALID: "ENUM_INVALID",
  DATE_SEQUENCE_INVALID: "DATE_SEQUENCE_INVALID",
  REQUIRED_FIELD_MISSING: "REQUIRED_FIELD_MISSING",
  NORMALIZATION_WARNING: "NORMALIZATION_WARNING",
  PROGRAM_MISMATCH: "PROGRAM_MISMATCH"
} as const;

9) Acceptance tests (Jest + supertest; one example)
it("PDF-OK-01: imports PDF, builds mapping, suppresses HOI outreach", async () => {
  const pdf = Buffer.from("..."); // test fixture
  const res = await request(app)
    .post("/imports")
    .set(authHdr)
    .field("type","pdf")
    .attach("file", pdf, "closing.pdf")
    .expect(202);

  const loc = res.headers.location;
  // simulate doc pipeline events here or use test doubles to run workers…

  // after extraction, mapping should show HOI
  const status = await request(app).get(loc).set(authHdr).expect(200);
  const hoi = status.body.mapping_preview.canonical.find((x:any)=>x.key==="HomeownersInsCarrier");
  expect(hoi.autofilled_from).toBe("document");

  // try to notify borrower for HOI -> blocked
  await request(app).post("/notifications")
    .set(authHdr)
    .send({ template_code: "BORR_HOI_REQUEST", loan_id: status.body.loan_id, to_party: "borrower", params: {} })
    .expect(409);
});

10) Operational defaults (config)

MAX_PDF_BYTES=524288000 (500MB), MAX_PAGES_PER_PDF=3000

IMPORT_MIN_CONFIDENCE=0.80

AMQP_PREFETCH_PER_WORKER=16

OCR_TIMEOUT_MS=180000 (with retry/backoff to import.pdf.validate.q.retry)

If you’d like, I can also generate the CSV template and JSON data dictionary files plus a YAML MISMO→canonical mapping starter so your team can commit these alongside the code.

You said:
I think the specifications need to be more explicit about how we envision ai integrated. How the aws ocr engine is integrated, technique for splitting documents, recovery, authoritative sources, etc.  Don't leave any part of this implementation to chance
ChatGPT said:
Thought for 7s

Absolutely. Here’s a no-ambiguity, engineering-grade AI integration spec that nails down OCR, splitting, extraction, recovery, “authoritative source” selection, and deterministic fallbacks. Anything marked [AI] is model-driven; everything else is deterministic. All paths use RabbitMQ (not Kafka) and enforce Do-Not-Ping (never ask humans for data that’s present or obtainable from docs/vendors).

0) Principles (hard rules)

Explainable by construction: every populated field must carry lineage (doc_id, page, bbox, text_hash, source_pointer, extractor_version).

Deterministic first, AI second: when rules or regexes can extract a value reliably, do that; use [AI] where ambiguity/variance demands it.

Authoritative source precedence: for each canonical key, choose from a ranked source list (below); conflicts resolved deterministically, then [AI] vote if still tied.

Idempotent orchestration: every worker is idempotent (content hash keys), bounded-retries → DLQ with explicit runbooks.

Do-Not-Ping enforced globally: any notification must pass canSatisfyFromDocsOrVendors() guard.

1) End-to-End Flow (events & workers)

Events (topic exchanges → queues → workers):

loan.docs#uploaded → SplitWorker → loan.docs#chunked

loan.docs#chunked → OcrWorker → loan.ocr#completed

loan.ocr#completed → ExtractWorker [AI] → loan.extract#completed

loan.extract#completed → ConflictWorker [AI] → loan.conflict#detected?

loan.extract#completed (or #conflict.resolved) → QcWorker → loan.qc#completed

loan.qc#completed → FinalizeWorker (if pass) → loan.finalize#completed

loan.finalize#completed → ExportWorker → loan.export#completed

Retry & DLQ: each queue has .retry.q (exponential backoff) and .dlq. Idempotency via (loan_id, stage, input_hash).

2) Document Splitting & Classification (DocChunker)
2.1 Inputs & Outputs

Input: a PDF (or ZIP of PDFs) with {loanId, importId?}.

Output: loan_documents rows per doc cluster (e.g., Note, Deed, CD, HOI), with page_range, doc_type, class_confidence, parent_pdf_id.

2.2 Hybrid Splitting Technique

Page feature extraction (deterministic):

Text header/footer shingles (LSH/simhash).

Keyphrase presence ("PROMISSORY NOTE", "Closing Disclosure", "Deed of Trust", "FLOOD HAZARD DETERMINATION", "Policy Declarations", "Uniform Residential Appraisal Report").

Layout fingerprints (page size/orientation, average font size, table density, signature block heuristics).

Clustering (deterministic + [AI] re-rank):

Greedy segmentation by bigrams of page features (cosine > θ_split triggers new cluster).

Merge small clusters (<2 pages) into nearest neighbor unless they match a short-form doc lexicon (e.g., addenda).

[AI] DocType re-rank: pass first page text to classifier prompt (few-shot); if classifier disagrees with deterministic type by >Δconf, mark for HITL.

Reordering & surgery:

Let users re-assign pages in Live Preview; changes produce a new chunk_plan_hash → re-run OCR only for impacted pages.

Key thresholds (configurable):

θ_split (cosine distance): 0.35

Min cluster pages: 2 (except known single-page docs)

Classifier acceptance margin Δconf: 0.18

Failure modes & recovery:

DOC_SPLIT_FAILED: fall back to per-page DocType guess [AI] + user surgery.

Password-protected PDF → error PDF_PASSWORD_REQUIRED.

3) AWS Textract OCR Integration
3.1 Mode & APIs

Synchronous for small pages (DetectDocumentText for simple, AnalyzeDocument with TABLES/FORMS for CDs/fees pages).

Asynchronous for large docs (StartDocumentAnalysis + GetDocumentAnalysis paginated).

We store raw blocks (word/line/table/key-value) in S3 (…/ocr/{doc_id}/{page}.json), plus a reflowed text and bbox map.

3.2 Performance & Limits

Concurrency caps per tenant (e.g., 8 async jobs).

Auto-deskew/denoise (OpenCV step) when Textract confidence < 0.80 on a page, then retry once.

Timeouts: 180s per page sync; 15m per async job.

Backoff: 1s → 2s → 4s; max 5 tries → .dlq.

3.3 Costs & Safeguards

Page budget per loan (default 3,000 pages).

“Heaviest first” policy disabled (process small clusters first for fast MappingPreview).

Cache by (pdf_sha256, page_index) to avoid re-OCR on re-uploads.

4) Extraction Engine [AI] (LLM + deterministic packs)
4.1 Prompt Packs (per doc type)

Templates live in /prompts/{doc_type}.md.

Each pack defines:

Schema (canonical keys + types + regex).

Anchors (phrases to look for) and negative anchors.

Few-shot examples (synthetic; redact PII).

Output contract: strict JSON schema; invalid → auto-repair pass.

4.2 Slicing & Context Chaining (token-safe)

Chunk long docs by semantic boundaries (e.g., fee tables, signature sections).

Provide JSON skeleton of already-extracted keys to each subsequent slice (context chaining).

Cap tokens per call; typical window 4–16k.

4.3 Deterministic Extractors (run before [AI])

Regex + positional rules for: NoteAmount, InterestRate, Term, FirstPaymentDate, MaturityDate, LateChargePct, GraceDays, Addresses, MIN, Policy Numbers, Flood Determination number, UCDP SSR IDs.

Tabular parsers for CDs (line items, totals).

Deterministic hits overwrite [AI] proposals unless conflict with higher authority source (see §6).

4.4 Merge & Voting

For each key:

Sources: deterministic, [AI] doc-specific, [AI] cross-doc, vendor (SSR/flood), payload (MISMO/CSV/JSON).

Score = w_src * conf_src; w_src comes from authority matrix; conf_src is extractor/LLM confidence.

Select max score; if within tie_margin (0.07) → create loan_conflicts item with [AI] rationale + “choose best” suggestion.

4.5 Confidence Computation

conf_src = 0.25*ocr_quality + 0.30*anchor_match + 0.25*value_regex_fit + 0.20*cross_doc_agreement

ocr_quality: mean Textract line confidence in the bbox region.

anchor_match: strength of nearby anchors (distance-weighted).

value_regex_fit: normalized Levenshtein to canonical format.

cross_doc_agreement: 1 if consistent across multiple docs, else fraction.

Thresholds:

Accept auto-populate if score ≥ 0.80.

Route to HITL if 0.60 ≤ score < 0.80.

Reject if < 0.60 (keep empty; QC will flag if required).

5) MISMO / CSV / JSON Import (payload path)

Validator normalizes payload and writes loan_datapoints(ingest_source='payload', autofilled_from='payload').

Downstream authority matrix (below) still applies; if payload conflicts with higher authority (e.g., Note), we record conflict and do not ping anyone.

6) Authoritative Source Matrix (examples)
Canonical Key	Source Priority (highest → lowest)
NoteAmount	Note(deterministic) → CD(TotalLoanAmount) → Payload(MISMO) → [AI] cross-doc
InterestRate	Note(deterministic) → Payload → [AI]
FirstPaymentDate	Note(deterministic) → CD → Payload → [AI]
MaturityDate	Note(deterministic) → Deed Rider → Payload → [AI]
Borrower Name	Note signature block → Deed/Mortgage → Payload → [AI]
Property Address	Deed → CD → Appraisal → Payload → [AI]
HOI Carrier/Policy	HOI Declarations → CD Insurance section → Payload → [AI]
Flood Zone	Flood Determination → Payload → [AI]
UCDP/SSR	Vendor SSR (vendor) → Appraisal addendum → Payload → [AI]

Each priority step carries authority_rank (e.g., Note=1, Deed=2…), feeding w_src.

Tie-breakers: most recent doc date > confidence > role override (admin).

7) Do-Not-Ping Guard (global)

Before any notification:

Query canonical values for required keys for that template.

If any key satisfied with autofilled_from in ('document','payload','vendor') and confidence ≥ 0.80 → block notification (409 DoNotPingPolicy) and record no_contact_reason='evidence_present' with lineage.

If not satisfied, attempt vendor backfill (e.g., SSR, flood) → if succeeds, persist ingest_source='vendor', block outreach.

Only then allow templated outreach.

8) Recovery & Resilience
8.1 Idempotency keys

SplitWorker: (pdf_sha256, chunk_plan_hash)

OcrWorker: (doc_id, page_sha256)

ExtractWorker: (doc_id, extractor_version, page_group_hash)

QcWorker: (loan_id, qc_rules_version, snapshot_hash)

8.2 Retries & DLQ

3 tries per stage (1m, 5m, 15m backoff) → DLQ with runbook code attached (see below).

Poison message detection: if same input hash DLQs > N in 24h, auto-raise Ops incident.

8.3 Partial failure handling

If OCR fails for some pages: extract from others; mark missing pages; QC will flag only if required fields depend on missing pages.

If extraction times out: save partial results + defects; HITL queue catches unresolved keys.

8.4 Runbooks (examples)

RB-OCR-TIMEOUT: Verify S3 object, re-trigger OCR with rasterize fallback; if still fail, route to HITL for manual keying of only required fields (keep Do-Not-Ping).

RB-DOC-MISCLASS: User performs page surgery; re-emit loan.docs#chunked.

RB-LOW-CONF: Nudge Escrow for specific missing doc only if program requires and no vendor path.

9) Data Contracts (pseudotypes)
// event: loan.docs#chunked
type ChunkedEvent = {
  loanId: string,
  importId?: string,
  clusters: Array<{
    docId: string,
    docType: 'NOTE'|'DEED'|'CD'|'HOI'|'FLOOD'|'APPRAISAL'|'TITLE'|'DISCLOSURE'|'OTHER',
    pages: number[],
    classConfidence: number
  }>,
  meta: { chunkPlanHash: string }
}

// extraction result item
type ExtractionItem = {
  key: string,                           // canonical key
  value: string|number|boolean|null,
  normalizedValue?: string,
  evidence: { docId: string, page: number, bbox?: number[], textHash: string },
  extractorVersion: string,
  source: 'deterministic'|'ai_doc'|'ai_cross'|'vendor'|'payload',
  confidence: number
}

10) Config (12-factor)
AMQP_URL=amqps://...
OCR_MODE=auto|sync|async
OCR_MAX_PAGES=3000
OCR_TIMEOUT_MS=180000
OCR_MIN_CONF_PAGE=0.80
EXTRACTOR_VERSION=v2025.09.01
CONF_ACCEPT=0.80
CONF_HITL=0.60
TIE_MARGIN=0.07
SPLIT_COSINE_THETA=0.35
CLASSIFIER_DELTA=0.18
PAGE_BUDGET_PER_LOAN=3000

11) Security & Privacy

Textract requests never include PII beyond the page image; all outputs stored in tenant-scoped buckets with KMS keys.

Postgres RLS per tenant; PII columns field-level encrypted; redacted previews in UI [AI] (blur SSNs, account numbers).

Immutable audit log on every extraction/override.

12) Acceptance Tests (must pass)

SPLIT-01: Mixed 180-page PDF auto-clusters into NOTE/DEED/CD/HOI; surgery updates chunk_plan_hash and downstream workers re-run only affected pieces.

OCR-02: Noisy scan → deskew+retry increases Textract confidence; if not, page flagged; extraction proceeds with partial.

EXTRACT-03 (Deterministic wins): NoteAmount from Note regex overrides conflicting [AI] value from CD.

AUTH-04: MaturityDate conflict (Note vs Payload) → Note wins; conflict recorded.

DNP-05: HOI found in Declarations (conf 0.92) → borrower HOI request blocked (409) with evidence pointer.

QC-06: TRID CD timing failure blocks Finalize with calendar math evidence.

REC-07: OCR worker fails 3x → DLQ; Ops runbook code injects; replay succeeds.

LINEAGE-08: Every exported field includes lineage doc/page/text_hash.

13) Concrete Code Hooks (where to put things)

/mnt/data/init-queues.ts: assert loan.docs, loan.ocr, loan.extract, loan.conflict, loan.qc, loan.finalize, loan.export + per-stage .retry/.dlq.

/mnt/data/service.ts: startSplitWorker(), startOcrWorker(), startExtractWorker(), startQcWorker().

/mnt/data/notification-service.ts: canSatisfyFromDocsOrVendors() + guarded sendNotificationGuarded().

/mnt/data/schema.ts: ensure loan_documents, loan_datapoints, loan_conflicts carry evidence_*, confidence, autofilled_from, ingest_source.

/mnt/data/topology.ts: AMQP connection, prefetch, backoff policies.

/mnt/data/auditService.ts: audit(stage, loanId, delta) on every state change.

14) What “no chance” looks like in practice

Every stage has thresholds and fallbacks (above), not “best effort.”

Every decision stores why (confidence, source, authority rank).

Every failure states who acts next (Auto-retry, HITL, or Ops), how (runbook), and when (SLA timers on the monitoring dashboard).

Every outreach is provably necessary (Do-Not-Ping audit row with evidence pointers).

If you want, I’ll generate:

The authority matrix as a versioned YAML your team can edit,

A starter prompt pack for Note, CD, HOI, Flood, Appraisal,

And TypeScript worker stubs (SplitWorker, OcrWorker, ExtractWorker) wired to Textract and your queues with the exact thresholds above.

You said:
generate:

The authority matrix as a versioned YAML your team can edit,

A starter prompt pack for Note, CD, HOI, Flood, Appraisal,

And TypeScript worker stubs (SplitWorker, OcrWorker, ExtractWorker) wired to Textract and your queues with the exact thresholds above.
ChatGPT said:

Perfect. Here are drop-in artifacts your team can commit today. Paths are suggestions; feel free to adjust.

1) Authority Matrix (versioned YAML)

File: config/authority-matrix.v2025-09-01.yaml

version: v2025-09-01
description: >
  Priority/weights used to select the authoritative value per canonical key.
  Higher weight wins; ties within tie_margin (0.07) create conflicts for HITL.

thresholds:
  accept_confidence: 0.80      # CONF_ACCEPT
  hitl_floor: 0.60             # CONF_HITL
  tie_margin: 0.07

sources:
  # global default weights (multiplied by extractor confidence)
  deterministic: 1.00
  note: 1.00
  deed: 0.95
  cd: 0.92
  hoi: 0.92
  flood: 0.95
  appraisal: 0.93
  title: 0.94
  payload: 0.85
  vendor: 0.97
  ai_doc: 0.75
  ai_cross: 0.78

keys:
  NoteAmount:
    priority:
      - {src: note, type: deterministic}
      - {src: cd, field: "TotalLoanAmount", type: deterministic}
      - {src: payload}
      - {src: ai_cross}
  InterestRate:
    priority:
      - {src: note, type: deterministic}
      - {src: payload}
      - {src: ai_cross}
  FirstPaymentDate:
    priority:
      - {src: note, type: deterministic}
      - {src: cd, type: deterministic}
      - {src: payload}
      - {src: ai_cross}
  MaturityDate:
    priority:
      - {src: note, type: deterministic}
      - {src: deed}
      - {src: payload}
      - {src: ai_cross}
  BorrowerFullName:
    priority:
      - {src: note}
      - {src: deed}
      - {src: payload}
      - {src: ai_cross}
  PropertyAddress:
    priority:
      - {src: deed}
      - {src: cd}
      - {src: appraisal}
      - {src: payload}
      - {src: ai_cross}
  HomeownersInsCarrier:
    priority:
      - {src: hoi}
      - {src: cd}
      - {src: payload}
      - {src: ai_cross}
  HOIPolicyNumber:
    priority:
      - {src: hoi, type: deterministic}
      - {src: cd}
      - {src: payload}
      - {src: ai_doc}
  HOIEffectiveDate:
    priority:
      - {src: hoi}
      - {src: payload}
      - {src: ai_doc}
  HOIExpirationDate:
    priority:
      - {src: hoi}
      - {src: payload}
      - {src: ai_doc}
  FloodZone:
    priority:
      - {src: flood, type: deterministic}
      - {src: payload}
      - {src: ai_doc}
  FloodInsRequired:
    priority:
      - {src: flood, type: deterministic}
      - {src: payload}
      - {src: ai_cross}
  UCDPSSRStatus:
    priority:
      - {src: vendor, field: "SSRStatus"}
      - {src: appraisal}
      - {src: payload}
      - {src: ai_doc}
  AppraisedValue:
    priority:
      - {src: appraisal, type: deterministic}
      - {src: payload}
      - {src: ai_doc}
  TitleCompanyName:
    priority:
      - {src: title}
      - {src: payload}
      - {src: ai_doc}

fallbacks:
  # when all sources fail or are below hitl_floor
  NoteAmount: { default: null, action: "HITL" }
  InterestRate: { default: null, action: "HITL" }
  PropertyAddress: { default: null, action: "HITL" }

do_not_ping:
  # If any of these are present above accept_confidence from document|payload|vendor,
  # suppress outreach for corresponding templates.
  suppress_if_present:
    - HomeownersInsCarrier
    - HOIPolicyNumber
    - HOIEffectiveDate
    - HOIExpirationDate
    - FloodZone
    - FloodInsRequired

2) Starter Prompt Pack (per doc type)

Directory: prompts/

These are minimal but production-safe. They enforce strict JSON and include anchors/negatives. Replace {{…}} with runtime inserts.

2.1 prompts/note.md
# ROLE
You extract loan terms from a Promissory Note. Return ONLY strict JSON.

# OUTPUT_SCHEMA (strict)
{
  "NoteAmount": "number",             // dollars
  "InterestRate": "number",           // percent e.g. 7.125
  "AmortTermMonths": "integer",
  "FirstPaymentDate": "YYYY-MM-DD",
  "MaturityDate": "YYYY-MM-DD",
  "LateChargePct": "number",
  "LateChargeGraceDays": "integer",
  "BorrowerFullName": "string"
}

# ANCHORS
- "PROMISSORY NOTE", "NOTE DATE", "ANNUAL INTEREST RATE", "LATE CHARGE", "FIRST PAYMENT", "MATURITY DATE"
# NEGATIVE_ANCHORS
- "Closing Disclosure", "Deed of Trust", "Insurance", "Flood"

# INSTRUCTIONS
- Prefer numeric values near the anchors above.
- Dates must be ISO YYYY-MM-DD; if month spelled, convert.
- If a value cannot be found with high confidence, set it to null.
- Do NOT hallucinate.
- Include `__evidence__` with page index and nearby text for each key.

# INPUT
{{DOC_TEXT_SLICE}}

# RETURN (JSON only)

2.2 prompts/cd.md
# ROLE
Extract closing economics from a Closing Disclosure (CD). JSON only.

# OUTPUT_SCHEMA
{
  "TotalLoanAmount": "number",
  "PAndIAmount": "number",
  "EscrowRequired": "boolean",
  "TaxEscrowMonthly": "number|null",
  "InsuranceEscrowMonthly": "number|null",
  "HOICarrier": "string|null",
  "HOIPolicyNumber": "string|null",
  "PropertyAddress": "string"
}

# ANCHORS
- "Closing Disclosure", "Projected Payments", "Escrow", "Insurance", "Policy"
# NEGATIVES
- "Promissory Note", "Deed", "Appraisal", "Flood Determination"

# RULES
- Monetary values: parse "$" and commas.
- Booleans: map "Yes/No" to true/false.
- Provide `__evidence__` for each field: page, line text.

# INPUT
{{DOC_TEXT_SLICE}}

2.3 prompts/hoi.md (Declarations page)
# ROLE
Extract homeowner's insurance (HOI) policy facts from a Policy Declarations page.

# OUTPUT_SCHEMA
{
  "HomeownersInsCarrier": "string",
  "HOIPolicyNumber": "string",
  "HOIEffectiveDate": "YYYY-MM-DD",
  "HOIExpirationDate": "YYYY-MM-DD",
  "HOIPhone": "string|null",
  "HOIEmail": "string|null"
}

# ANCHORS
- "DECLARATIONS", "POLICY NUMBER", "EFFECTIVE", "EXPIRATION", "CARRIER", "INSURANCE COMPANY", "CONTACT"
# RULES
- Normalize dates to ISO.
- Strip spaces/dashes around policy numbers.
- Return `__evidence__` per field (page, snippet).

# INPUT
{{DOC_TEXT_SLICE}}

2.4 prompts/flood.md
# ROLE
Extract flood determination facts.

# OUTPUT_SCHEMA
{
  "FloodZone": "string",
  "FloodInsRequired": "boolean",
  "DeterminationIdentifier": "string|null"
}

# ANCHORS
- "FLOOD HAZARD DETERMINATION", "SFHA", "ZONE", "REQUIRED"

# RULES
- Map "IN/OUT OF SFHA" accordingly.
- Evidence per field.

# INPUT
{{DOC_TEXT_SLICE}}

2.5 prompts/appraisal.md
# ROLE
Extract appraisal facts.

# OUTPUT_SCHEMA
{
  "AppraisalDate": "YYYY-MM-DD",
  "AppraisedValue": "number",
  "AppraisalFormType": "string"
}

# ANCHORS
- "Uniform Residential Appraisal Report", "URAR", "Effective Date of Appraisal", "Appraised Value"

# RULES
- Value is dollars; remove commas.
- Evidence per field.

# INPUT
{{DOC_TEXT_SLICE}}

3) TypeScript Worker Stubs (wired to RabbitMQ + Textract)

Directory: src/workers/

Assumes you already have topology.ts (AMQP connection), storage.ts (S3), repo.ts, and auditService.ts. Environment thresholds match the spec.

3.1 src/workers/SplitWorker.ts
import { mq } from "../topology";
import { repo } from "../repo";
import { audit } from "../auditService";
import { cosineChunkPlan, classifyDocTypes } from "../utils/docchunker"; // implement deterministic features here

const SPLIT_COSINE_THETA = Number(process.env.SPLIT_COSINE_THETA ?? "0.35");
const CLASSIFIER_DELTA   = Number(process.env.CLASSIFIER_DELTA ?? "0.18");

export async function startSplitWorker() {
  await mq.consume("loan.docs#uploaded", async (msg, ch) => {
    const { loanId, importId, fileUri, meta } = JSON.parse(msg.content.toString());
    try {
      const plan = await cosineChunkPlan(fileUri, { theta: SPLIT_COSINE_THETA });
      const clusters = await classifyDocTypes(plan, { delta: CLASSIFIER_DELTA }); // returns [{docId, docType, pages, classConfidence}]
      await repo.persistClusters(loanId, clusters, { parentUri: fileUri, chunkPlanHash: plan.hash });
      await mq.publish("loan.docs", "chunked", { loanId, importId, clusters, meta: { chunkPlanHash: plan.hash, importId } });
      await audit("doc.split.completed", loanId, { clustersCount: clusters.length });
      ch.ack(msg);
    } catch (e) {
      await repo.recordSplitError(loanId, e);
      await mq.publish("loan.docs", "split.failed", { loanId, importId, error: String(e) });
      ch.nack(msg, false, false); // to DLQ via x-dead-letter-exchange
    }
  });
}

3.2 src/workers/OcrWorker.ts
import { mq } from "../topology";
import { repo } from "../repo";
import { audit } from "../auditService";
import { textractAnalyze, rasterizeAndRetry } from "../utils/textract";

const OCR_MIN_CONF_PAGE = Number(process.env.OCR_MIN_CONF_PAGE ?? "0.80");
const OCR_TIMEOUT_MS    = Number(process.env.OCR_TIMEOUT_MS ?? "180000");

export async function startOcrWorker() {
  await mq.consume("loan.docs#chunked", async (msg, ch) => {
    const { loanId, importId, clusters, meta } = JSON.parse(msg.content.toString());
    try {
      for (const c of clusters) {
        for (const page of c.pages) {
          const res = await textractAnalyze({ docId: c.docId, page, timeoutMs: OCR_TIMEOUT_MS }); // stores blocks in S3, returns {confidence}
          if (res.confidence < OCR_MIN_CONF_PAGE) {
            const res2 = await rasterizeAndRetry({ docId: c.docId, page, timeoutMs: OCR_TIMEOUT_MS });
            if (res2.confidence < OCR_MIN_CONF_PAGE) await repo.flagLowOcrPage(c.docId, page, res2.confidence);
          }
        }
      }
      await mq.publish("loan.ocr", "completed", { loanId, importId, clusters, meta });
      await audit("doc.ocr.completed", loanId, { clusterCount: clusters.length });
      ch.ack(msg);
    } catch (e) {
      await repo.recordOcrError(loanId, e);
      ch.nack(msg, false, false);
    }
  });
}

3.3 src/workers/ExtractWorker.ts
import { mq } from "../topology";
import { repo } from "../repo";
import { audit } from "../auditService";
import { runPromptPack, deterministicExtract } from "../utils/extractors";
import { loadAuthorityMatrix } from "../utils/authority";

const EXTRACTOR_VERSION = process.env.EXTRACTOR_VERSION ?? "v2025.09.01";
const CONF_ACCEPT       = Number(process.env.CONF_ACCEPT ?? "0.80");
const CONF_HITL         = Number(process.env.CONF_HITL ?? "0.60");

export async function startExtractWorker() {
  const matrix = await loadAuthorityMatrix("config/authority-matrix.v2025-09-01.yaml");

  await mq.consume("loan.ocr#completed", async (msg, ch) => {
    const { loanId, importId, clusters, meta } = JSON.parse(msg.content.toString());
    try {
      const items:any[] = [];

      // 1) deterministic first
      for (const c of clusters) {
        const det = await deterministicExtract(c.docId, c.docType); // returns ExtractionItem[]
        items.push(...det.map(x => ({ ...x, source: "deterministic", extractorVersion: EXTRACTOR_VERSION })));
      }

      // 2) AI per doc-type (only for keys not confidently set)
      for (const c of clusters) {
        const pendingKeys = await repo.pendingKeysForDoc(loanId, c.docType, CONF_ACCEPT);
        if (!pendingKeys.length) continue;
        const ai = await runPromptPack(c.docId, c.docType, pendingKeys); // returns ExtractionItem[] with __evidence__
        items.push(...ai.map(x => ({ ...x, source: "ai_doc", extractorVersion: EXTRACTOR_VERSION })));
      }

      // 3) Merge & vote per canonical key using authority matrix
      const merged = await repo.mergeByAuthority(loanId, items, matrix);
      // merged returns [{key,value,normalizedValue,confidence,autofilled_from='document', evidence:{docId,page,textHash}, winnerSource}]

      // 4) Persist datapoints + conflicts
      await repo.persistDatapoints(loanId, merged, { ingest_source: "document", extractor_version: EXTRACTOR_VERSION });
      const conflicts = merged.filter((m:any) => m.isConflict && m.maxScore - m.nextScore < matrix.thresholds.tie_margin);
      if (conflicts.length) await repo.createConflicts(loanId, conflicts);

      // 5) Publish next stage
      await mq.publish("loan.extract", "completed", { loanId, importId, counts: { items: merged.length, conflicts: conflicts.length }, meta });
      await audit("extract.completed", loanId, { extractorVersion: EXTRACTOR_VERSION, items: merged.length, conflicts: conflicts.length });
      ch.ack(msg);
    } catch (e) {
      await repo.recordExtractError(loanId, e);
      ch.nack(msg, false, false);
    }
  });
}

Utility Skeletons (you can fill in)

src/utils/docchunker.ts

export async function cosineChunkPlan(fileUri: string, { theta }: { theta: number }) {
  // 1) read pages → features (headers, shingles, page dims)
  // 2) greedy segment where cosine distance > theta
  // 3) return { hash, segments:[{start,end}] }
  return { hash: "chunkplan-…", segments: [] };
}

export async function classifyDocTypes(plan:any, { delta }: { delta:number }) {
  // For each segment's first page, do deterministic keyword match → baseType
  // Optionally re-rank with a small LLM classifier; if |p_ai - p_det| > delta, mark needs_review
  return [{ docId: "uuid", docType: "NOTE" as const, pages: [1,2,3], classConfidence: 0.93 }];
}


src/utils/textract.ts

import { TextractClient, AnalyzeDocumentCommand } from "@aws-sdk/client-textract";

const client = new TextractClient({ region: process.env.AWS_REGION });

export async function textractAnalyze({ docId, page, timeoutMs }:{docId:string,page:number,timeoutMs:number}) {
  // load page image/PDF page from storage, call AnalyzeDocument with TABLES+FORMS when appropriate
  // persist raw blocks to S3; compute mean confidence
  return { confidence: 0.91 };
}

export async function rasterizeAndRetry(opts:any) {
  // convert to 300dpi grayscale, deskew; retry AnalyzeDocument
  return { confidence: 0.88 };
}


src/utils/extractors.ts

export async function deterministicExtract(docId: string, docType: string) {
  // Use regex/positional rules per docType; return ExtractionItem[]
  return [];
}

export async function runPromptPack(docId: string, docType: string, keys: string[]) {
  // Fetch OCR text slices; call LLM with prompts/{docType}.md; coerce to strict schema
  return [];
}


src/utils/authority.ts

import fs from "fs";
import yaml from "js-yaml";
export async function loadAuthorityMatrix(path: string) {
  return yaml.load(fs.readFileSync(path, "utf8")) as any;
}

Wiring in service.ts
import { startSplitWorker } from "./workers/SplitWorker";
import { startOcrWorker } from "./workers/OcrWorker";
import { startExtractWorker } from "./workers/ExtractWorker";

export async function startWorkers() {
  await Promise.all([
    startSplitWorker(),
    startOcrWorker(),
    startExtractWorker(),
    // … startQcWorker(), etc.
  ]);
}

Env (example .env)
AMQP_URL=amqps://…
AWS_REGION=us-east-1
SPLIT_COSINE_THETA=0.35
CLASSIFIER_DELTA=0.18
OCR_MIN_CONF_PAGE=0.80
OCR_TIMEOUT_MS=180000
EXTRACTOR_VERSION=v2025.09.01
CONF_ACCEPT=0.80
CONF_HITL=0.60


If you want, I can also generate a ready-to-parse JSON Schema for the prompt outputs (so we can auto-validate every LLM response), and a repo migration to add the prompt_version/extractor_version columns to loan_datapoints for full lineage.

You said:
generate a ready-to-parse JSON Schema for the prompt outputs (so we can auto-validate every LLM response), and a repo migration to add the prompt_version/extractor_version columns to loan_datapoints for full lineage.
ChatGPT said:
Thought for 8s

Locked and loaded. Below are drop-in artifacts you can commit today:

Ready-to-parse JSON Schemas (Draft 2020-12) for each prompt output (Note, CD, HOI, Flood, Appraisal) + a union schema.

A SQL migration adding prompt_version and extractor_version to loan_datapoints (with helpful indexes) for full lineage.

I’ve kept the response structure identical across all doc types:

{
  "docType": "NOTE|CD|HOI|FLOOD|APPRAISAL",
  "promptVersion": "vYYYY-MM-DD.tag",
  "data": { /* doc-specific keys */ },
  "evidence": {
    "FieldName": { "docId":"uuid", "page":1, "bbox":[x1,y1,x2,y2], "textHash":"sha256", "snippet":"..." }
  }
}


Additional properties are not allowed; nullability is explicit; numbers are typed.

1) JSON Schemas
1.1 Union schema

File: schemas/prompt-outputs/union.schema.json

{
  "$id": "https://loanserve.ai/schemas/prompt-outputs/union.schema.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "LLM Prompt Output (Union)",
  "type": "object",
  "oneOf": [
    { "$ref": "note.schema.json" },
    { "$ref": "cd.schema.json" },
    { "$ref": "hoi.schema.json" },
    { "$ref": "flood.schema.json" },
    { "$ref": "appraisal.schema.json" }
  ]
}

Evidence definition (reused via $defs)

Copy this $defs block into each doc-specific schema (shown below once for readability):

"$defs": {
  "Evidence": {
    "type": "object",
    "additionalProperties": false,
    "properties": {
      "docId":   { "type": "string", "format": "uuid" },
      "page":    { "type": "integer", "minimum": 1 },
      "bbox":    { "type": "array", "items": { "type": "number" }, "minItems": 4, "maxItems": 4 },
      "textHash":{ "type": "string", "pattern": "^[A-Fa-f0-9]{64}$" },
      "snippet": { "type": "string", "maxLength": 1000 }
    },
    "required": ["docId", "page", "textHash"]
  }
}

1.2 Note schema

File: schemas/prompt-outputs/note.schema.json

{
  "$id": "https://loanserve.ai/schemas/prompt-outputs/note.schema.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "Promissory Note Extraction",
  "type": "object",
  "additionalProperties": false,
  "$defs": {
    "Evidence": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "docId":   { "type": "string", "format": "uuid" },
        "page":    { "type": "integer", "minimum": 1 },
        "bbox":    { "type": "array", "items": { "type": "number" }, "minItems": 4, "maxItems": 4 },
        "textHash":{ "type": "string", "pattern": "^[A-Fa-f0-9]{64}$" },
        "snippet": { "type": "string", "maxLength": 1000 }
      },
      "required": ["docId", "page", "textHash"]
    }
  },
  "properties": {
    "docType": { "const": "NOTE" },
    "promptVersion": { "type": "string", "pattern": "^v\\d{4}-\\d{2}-\\d{2}(?:[.-][A-Za-z0-9]+)?$" },
    "data": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "NoteAmount":          { "type": ["number","null"], "minimum": 0 },
        "InterestRate":        { "type": ["number","null"], "minimum": 0, "maximum": 100 },
        "AmortTermMonths":     { "type": ["integer","null"], "minimum": 1, "maximum": 480 },
        "FirstPaymentDate":    { "type": ["string","null"], "format": "date" },
        "MaturityDate":        { "type": ["string","null"], "format": "date" },
        "LateChargePct":       { "type": ["number","null"], "minimum": 0, "maximum": 100 },
        "LateChargeGraceDays": { "type": ["integer","null"], "minimum": 0, "maximum": 60 },
        "BorrowerFullName":    { "type": ["string","null"], "minLength": 1, "maxLength": 200 }
      },
      "required": ["NoteAmount","InterestRate","AmortTermMonths","FirstPaymentDate","MaturityDate","BorrowerFullName"]
    },
    "evidence": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "NoteAmount":          { "$ref": "#/$defs/Evidence" },
        "InterestRate":        { "$ref": "#/$defs/Evidence" },
        "AmortTermMonths":     { "$ref": "#/$defs/Evidence" },
        "FirstPaymentDate":    { "$ref": "#/$defs/Evidence" },
        "MaturityDate":        { "$ref": "#/$defs/Evidence" },
        "LateChargePct":       { "$ref": "#/$defs/Evidence" },
        "LateChargeGraceDays": { "$ref": "#/$defs/Evidence" },
        "BorrowerFullName":    { "$ref": "#/$defs/Evidence" }
      }
    }
  },
  "required": ["docType","promptVersion","data","evidence"]
}

1.3 Closing Disclosure (CD) schema

File: schemas/prompt-outputs/cd.schema.json

{
  "$id": "https://loanserve.ai/schemas/prompt-outputs/cd.schema.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "Closing Disclosure Extraction",
  "type": "object",
  "additionalProperties": false,
  "$defs": { "$ref": "note.schema.json#/$defs" },
  "properties": {
    "docType": { "const": "CD" },
    "promptVersion": { "type": "string", "pattern": "^v\\d{4}-\\d{2}-\\d{2}(?:[.-][A-Za-z0-9]+)?$" },
    "data": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "TotalLoanAmount":        { "type": ["number","null"], "minimum": 0 },
        "PAndIAmount":            { "type": ["number","null"], "minimum": 0 },
        "EscrowRequired":         { "type": ["boolean","null"] },
        "TaxEscrowMonthly":       { "type": ["number","null"], "minimum": 0 },
        "InsuranceEscrowMonthly": { "type": ["number","null"], "minimum": 0 },
        "HOICarrier":             { "type": ["string","null"], "minLength": 1, "maxLength": 200 },
        "HOIPolicyNumber":        { "type": ["string","null"], "minLength": 1, "maxLength": 120 },
        "PropertyAddress":        { "type": ["string","null"], "minLength": 1, "maxLength": 300 }
      },
      "required": ["TotalLoanAmount","PAndIAmount","EscrowRequired","PropertyAddress"]
    },
    "evidence": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "TotalLoanAmount":        { "$ref": "note.schema.json#/$defs/Evidence" },
        "PAndIAmount":            { "$ref": "note.schema.json#/$defs/Evidence" },
        "EscrowRequired":         { "$ref": "note.schema.json#/$defs/Evidence" },
        "TaxEscrowMonthly":       { "$ref": "note.schema.json#/$defs/Evidence" },
        "InsuranceEscrowMonthly": { "$ref": "note.schema.json#/$defs/Evidence" },
        "HOICarrier":             { "$ref": "note.schema.json#/$defs/Evidence" },
        "HOIPolicyNumber":        { "$ref": "note.schema.json#/$defs/Evidence" },
        "PropertyAddress":        { "$ref": "note.schema.json#/$defs/Evidence" }
      }
    }
  },
  "required": ["docType","promptVersion","data","evidence"]
}

1.4 HOI (Declarations) schema

File: schemas/prompt-outputs/hoi.schema.json

{
  "$id": "https://loanserve.ai/schemas/prompt-outputs/hoi.schema.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "Homeowners Insurance Declarations Extraction",
  "type": "object",
  "additionalProperties": false,
  "$defs": { "$ref": "note.schema.json#/$defs" },
  "properties": {
    "docType": { "const": "HOI" },
    "promptVersion": { "type": "string", "pattern": "^v\\d{4}-\\d{2}-\\d{2}(?:[.-][A-Za-z0-9]+)?$" },
    "data": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "HomeownersInsCarrier": { "type": ["string","null"], "minLength": 1, "maxLength": 200 },
        "HOIPolicyNumber":      { "type": ["string","null"], "minLength": 1, "maxLength": 120 },
        "HOIEffectiveDate":     { "type": ["string","null"], "format": "date" },
        "HOIExpirationDate":    { "type": ["string","null"], "format": "date" },
        "HOIPhone":             { "type": ["string","null"], "maxLength": 40 },
        "HOIEmail":             { "type": ["string","null"], "format": "email" }
      },
      "required": ["HomeownersInsCarrier","HOIPolicyNumber","HOIEffectiveDate","HOIExpirationDate"]
    },
    "evidence": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "HomeownersInsCarrier": { "$ref": "note.schema.json#/$defs/Evidence" },
        "HOIPolicyNumber":      { "$ref": "note.schema.json#/$defs/Evidence" },
        "HOIEffectiveDate":     { "$ref": "note.schema.json#/$defs/Evidence" },
        "HOIExpirationDate":    { "$ref": "note.schema.json#/$defs/Evidence" },
        "HOIPhone":             { "$ref": "note.schema.json#/$defs/Evidence" },
        "HOIEmail":             { "$ref": "note.schema.json#/$defs/Evidence" }
      }
    }
  },
  "required": ["docType","promptVersion","data","evidence"]
}

1.5 Flood schema

File: schemas/prompt-outputs/flood.schema.json

{
  "$id": "https://loanserve.ai/schemas/prompt-outputs/flood.schema.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "Flood Determination Extraction",
  "type": "object",
  "additionalProperties": false,
  "$defs": { "$ref": "note.schema.json#/$defs" },
  "properties": {
    "docType": { "const": "FLOOD" },
    "promptVersion": { "type": "string", "pattern": "^v\\d{4}-\\d{2}-\\d{2}(?:[.-][A-Za-z0-9]+)?$" },
    "data": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "FloodZone":             { "type": ["string","null"], "minLength": 1, "maxLength": 40 },
        "FloodInsRequired":      { "type": ["boolean","null"] },
        "DeterminationIdentifier": { "type": ["string","null"], "maxLength": 120 }
      },
      "required": ["FloodZone","FloodInsRequired"]
    },
    "evidence": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "FloodZone":               { "$ref": "note.schema.json#/$defs/Evidence" },
        "FloodInsRequired":        { "$ref": "note.schema.json#/$defs/Evidence" },
        "DeterminationIdentifier": { "$ref": "note.schema.json#/$defs/Evidence" }
      }
    }
  },
  "required": ["docType","promptVersion","data","evidence"]
}

1.6 Appraisal schema

File: schemas/prompt-outputs/appraisal.schema.json

{
  "$id": "https://loanserve.ai/schemas/prompt-outputs/appraisal.schema.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "Appraisal Extraction",
  "type": "object",
  "additionalProperties": false,
  "$defs": { "$ref": "note.schema.json#/$defs" },
  "properties": {
    "docType": { "const": "APPRAISAL" },
    "promptVersion": { "type": "string", "pattern": "^v\\d{4}-\\d{2}-\\d{2}(?:[.-][A-Za-z0-9]+)?$" },
    "data": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "AppraisalDate":    { "type": ["string","null"], "format": "date" },
        "AppraisedValue":   { "type": ["number","null"], "minimum": 0 },
        "AppraisalFormType":{ "type": ["string","null"], "maxLength": 120 }
      },
      "required": ["AppraisalDate","AppraisedValue"]
    },
    "evidence": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "AppraisalDate":     { "$ref": "note.schema.json#/$defs/Evidence" },
        "AppraisedValue":    { "$ref": "note.schema.json#/$defs/Evidence" },
        "AppraisalFormType": { "$ref": "note.schema.json#/$defs/Evidence" }
      }
    }
  },
  "required": ["docType","promptVersion","data","evidence"]
}

(Optional) AJV validator snippet

File: src/validation/promptOutputValidator.ts

import Ajv from "ajv";
import addFormats from "ajv-formats";
import unionSchema from "../../schemas/prompt-outputs/union.schema.json";
import noteSchema from "../../schemas/prompt-outputs/note.schema.json";
import cdSchema from "../../schemas/prompt-outputs/cd.schema.json";
import hoiSchema from "../../schemas/prompt-outputs/hoi.schema.json";
import floodSchema from "../../schemas/prompt-outputs/flood.schema.json";
import appraisalSchema from "../../schemas/prompt-outputs/appraisal.schema.json";

const ajv = new Ajv({ allErrors: true, strict: true });
addFormats(ajv);

[ noteSchema, cdSchema, hoiSchema, floodSchema, appraisalSchema ].forEach(s => ajv.addSchema(s));
const validate = ajv.compile(unionSchema);

export function assertValidPromptOutput(obj: unknown) {
  const ok = validate(obj);
  if (!ok) {
    const err = new Error("PromptOutputValidationError");
    (err as any).details = validate.errors;
    throw err;
  }
}

2) SQL Migration — lineage columns in loan_datapoints

File: migrations/002_add_prompt_extractor_version.sql

-- 002_add_prompt_extractor_version.sql
-- Purpose: add prompt_version + extractor_version to loan_datapoints for full lineage,
-- and helpful indexes for analytics.

BEGIN;

-- Add columns if absent
DO $$
BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM information_schema.columns
    WHERE table_name='loan_datapoints' AND column_name='prompt_version'
  ) THEN
    ALTER TABLE loan_datapoints
      ADD COLUMN prompt_version text NULL;
  END IF;

  IF NOT EXISTS (
    SELECT 1 FROM information_schema.columns
    WHERE table_name='loan_datapoints' AND column_name='extractor_version'
  ) THEN
    ALTER TABLE loan_datapoints
      ADD COLUMN extractor_version text NULL;
  END IF;

  -- Optional: who/when the prompt/extractor produced this row
  IF NOT EXISTS (
    SELECT 1 FROM information_schema.columns
    WHERE table_name='loan_datapoints' AND column_name='produced_at'
  ) THEN
    ALTER TABLE loan_datapoints
      ADD COLUMN produced_at timestamptz NULL DEFAULT now();
  END IF;
END $$;

-- Helpful partial indexes
CREATE INDEX IF NOT EXISTS idx_ldp_key_extractor
  ON loan_datapoints (key, extractor_version);

CREATE INDEX IF NOT EXISTS idx_ldp_prompt_version
  ON loan_datapoints (prompt_version)
  WHERE prompt_version IS NOT NULL;

-- (Optional) tighten NOT NULL later, after backfill:
-- ALTER TABLE loan_datapoints ALTER COLUMN extractor_version SET NOT NULL;

COMMIT;

Persistence convention (use in your ExtractWorker)

When writing LLM results:

loan_datapoints.prompt_version = payload.promptVersion

loan_datapoints.extractor_version = process.env.EXTRACTOR_VERSION (e.g., v2025.09.01)

loan_datapoints.produced_at = now()

For deterministic extracts:

prompt_version = NULL

extractor_version = EXTRACTOR_VERSION

For imports (payload/MISMO/CSV/JSON):

Set extractor_version = 'payload' (or a fixed tag)

prompt_version = NULL

Where to wire this

ExtractWorker: after schema validation of LLM JSON, persist prompt_version and extractor_version.

repo.mergeByAuthority / persistDatapoints: accept and store these two fields.

Exports/QC lineage: include versions in lineage metadata so you can reproduce outputs exactly.